[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [7, 12], [7, 13], [7, 14], [10, 15], [10, 16], [10, 17], [6, 18], [6, 19], [6, 20], [5, 21], [5, 22], [5, 23], [12, 24], [12, 25], [12, 26], [9, 27], [9, 28], [9, 29], [4, 30], [4, 31], [4, 32], [11, 33], [11, 34], [11, 35], [8, 36], [8, 37], [8, 38], [16, 39], [16, 40], [16, 41], [13, 42], [15, 43], [15, 44], [15, 45], [14, 46], [14, 47], [14, 48]], [[0, 1], [1, 3], [2, 2], [3, 7], [4, 10], [5, 6], [6, 5], [7, 12], [8, 9], [9, 4], [10, 11], [11, 8], [12, 16], [18, 13], [27, 15], [33, 14]], {"0": {"ucb": 1.5018372456273477, "p_ucb": 1.405653521024113, "var_p_ucb": 1.628476939577778, "action": "Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\n", "prob": 0.9}, "1": {"ucb": 1.7221434532544055, "p_ucb": 1.4977147625135245, "var_p_ucb": 1.7287909002728807, "action": "Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\n", "prob": 0.8}, "2": {"ucb": 1.7221434532544055, "p_ucb": 1.5538219351987448, "var_p_ucb": 1.799340331568061, "action": "Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\n", "prob": 0.85}, "3": {"ucb": 2.4599451117892777, "p_ucb": 2.180953344953386, "var_p_ucb": 2.4132691685185117, "action": "Clue of Step 2:Identify the specific sub-tasks required to classify the content of the audio file, such as audio feature extraction and audio classification.\n", "prob": 0.85}, "4": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Establish the order of operations; determine what comes first, like preprocessing the audio, and then what follows, such as classifying the processed audio.\n", "prob": 0.8}, "5": {"ucb": 2.4599451117892777, "p_ucb": 2.27395060056535, "var_p_ucb": 2.519932060810777, "action": "Clue of Step 2:Ensure that the output of one sub-task feeds logically into the next sub-task, maintaining a coherent flow throughout the process.\n", "prob": 0.9}, "6": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Identify the necessary input for the audio classification task, focusing on the required audio file format and possible classification labels.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Outline the specific steps to process the audio data, ensuring the classification algorithm will produce accurate results.\n", "prob": 0.8}, "8": {"ucb": 2.3847549316111336, "p_ucb": 2.1170416918019637, "var_p_ucb": 2.318287986788977, "action": "Clue of Step 2:Determine the types of audio features that need to be extracted and how they relate to the chosen classification model.\n", "prob": 0.85}, "9": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Identify the specific audio classification task that needs to be done, such as recognizing commands or identifying emotions in the audio.\n", "prob": 0.9}, "10": {"ucb": 2.3847549316111336, "p_ucb": 2.027803945198907, "var_p_ucb": 2.2172122228337434, "action": "Clue of Step 2:Break down the audio classification task into specific input and output requirements, ensuring that they conform to the definitions of audio classification.\n", "prob": 0.8}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Establish the sequence of operations needed to complete the audio classification, emphasizing any dependencies that may exist between the input processing and the classification step.\n", "prob": 0.85}, "12": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:First sub-task should involve extracting features from the audio file to prepare for classification.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:The second sub-task should be classifying the extracted features to determine the type of audio content.\n", "prob": 0.85}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that the output from the classification sub-task is formatted correctly for end-user clarity.\n", "prob": 0.8}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Start by recording the input audio data, ensuring it is in a suitable format for analysis.\n", "prob": 0.9}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Process the audio to extract relevant acoustic features that will be useful for classification.\n", "prob": 0.8}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Utilize an audio classification model to assign categories or labels to the processed audio.\n", "prob": 0.9}, "18": {"ucb": 2.69629414748641, "p_ucb": 2.69629414748641, "var_p_ucb": 2.8726303426294413, "action": "\n", "prob": 1.0}, "19": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "20": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "21": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Determine the order of execution for the tasks based on their dependencies, ensuring the output of one sub-task serves as input for subsequent tasks.\n", "prob": 0.9}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that all sub-tasks selected are necessary for completing the overall request and do not introduce redundant steps or unnecessary complexity.\n", "prob": 0.85}, "23": {"ucb": 3.3302184446307908, "p_ucb": 3.0638009690603276, "var_p_ucb": 3.2647589409909026, "action": "Clue of Step 3:Review the expected outputs of each task to confirm they align with the required outputs for the user request, ensuring cohesive integration throughout the process.\n", "prob": 0.92}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific audio classification model to use based on the type of audio being classified and the available classes for classification.\n", "prob": 0.9}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Prepare the input audio by ensuring it is correctly formatted, and then invoke the audio classification tool passing the audio data.\n", "prob": 0.85}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Evaluate the classification results to ensure they meet the expected criteria and accurately represent the input audio.\n", "prob": 0.8}, "27": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Identify the audio classification model to use based on the audio file's expected content and classification objectives.\n", "prob": 0.9}, "28": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that the input audio file is pre-processed to match the expected format and features required by the audio classification model.\n", "prob": 0.8}, "29": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "Clue of Step 3:Establish a feedback loop to validate the accuracy of the classification results and adjust model parameters if necessary.\n", "prob": 0.7}, "30": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "31": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "32": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "33": {"ucb": 2.69629414748641, "p_ucb": 2.69629414748641, "var_p_ucb": 2.8726303426294413, "action": "\n", "prob": 1.0}, "34": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "35": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Identify the necessary processing steps to convert the audio input into features that can be classified.\n", "prob": 0.85}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.497663833473093, "var_p_ucb": 2.6614882671121487, "action": "Clue of Step 3:Select an appropriate audio classification model that aligns with the input type and expected output.\n", "prob": 0.75}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Define how the output labels will be presented after the classification is complete and ensure they are clear and meaningful.\n", "prob": 0.8}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Utilize Audio Classification to assign a label or class based on the audio features extracted in the previous step, accurately classifying the content.\n", "prob": 0.9}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Ensure that the output from the Audio Classification task provides a clear and concise classification, which encompasses all the relevant aspects of the audio content.\n", "prob": 0.8}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Integrate feedback from the classification results with the original audio analysis to verify the accuracy and relevancy of the classification.\n", "prob": 0.85}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the selected subtasks collectively address the objective of classifying the audio content and that all necessary steps for classification are included.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Verify that the selected audio classification model fits the user request by checking whether it can handle the specific labels or classes relevant to the audio file content.\n", "prob": 0.9}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Test the audio classification model on a sample of the audio data to ensure compatibility and expected output.\n", "prob": 0.85}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Review the results of the audio classification to confirm accuracy and adjust the model or parameters if necessary.\n", "prob": 0.8}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Identify the key requirements for performing audio classification and define the input and output expectations for the selected task.\n", "prob": 0.85}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Ensure that the necessary resources and model capabilities for audio classification are available and aligned with user request.\n", "prob": 0.8}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.497663833473093, "var_p_ucb": 2.6614882671121487, "action": "Clue of Step 4:Plan for any potential preprocessing that may be needed for the audio input before classification can occur.\n", "prob": 0.75}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\n", "value": 0.6666666661666666}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\n", "value": 0.6666666661666666}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\nClue of Step 2:Identify the specific sub-tasks required to classify the content of the audio file, such as audio feature extraction and audio classification.\n", "value": 0.6666666661666666}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\nClue of Step 2:Establish the order of operations; determine what comes first, like preprocessing the audio, and then what follows, such as classifying the processed audio.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\nClue of Step 2:Ensure that the output of one sub-task feeds logically into the next sub-task, maintaining a coherent flow throughout the process.\n", "value": 0.6666666661666666}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\nClue of Step 2:Identify the necessary input for the audio classification task, focusing on the required audio file format and possible classification labels.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\nClue of Step 2:Outline the specific steps to process the audio data, ensuring the classification algorithm will produce accurate results.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\nClue of Step 2:Determine the types of audio features that need to be extracted and how they relate to the chosen classification model.\n", "value": 0.6666666661666666}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\nClue of Step 2:Identify the specific audio classification task that needs to be done, such as recognizing commands or identifying emotions in the audio.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\nClue of Step 2:Break down the audio classification task into specific input and output requirements, ensuring that they conform to the definitions of audio classification.\n", "value": 0.6666666661666666}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\nClue of Step 2:Establish the sequence of operations needed to complete the audio classification, emphasizing any dependencies that may exist between the input processing and the classification step.\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\nClue of Step 2:Identify the specific sub-tasks required to classify the content of the audio file, such as audio feature extraction and audio classification.\nClue of Step 3:First sub-task should involve extracting features from the audio file to prepare for classification.\n", "value": 0.6666666661666666}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal which is to classify the content of the provided audio file, and break it down into subtasks such as analyzing audio features and outputting classifications.\nClue of Step 2:Ensure that the output of one sub-task feeds logically into the next sub-task, maintaining a coherent flow throughout the process.\n\n", "value": 0.6666666661666666}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the specific requirements of the audio classification task, such as determining the audio type and potential classes available for classification.\nClue of Step 2:Determine the types of audio features that need to be extracted and how they relate to the chosen classification model.\nClue of Step 3:Identify the audio classification model to use based on the audio file's expected content and classification objectives.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure to outline the data type of the input (in this case, audio) and determine how to process this data to achieve the classification goal effectively.\nClue of Step 2:Break down the audio classification task into specific input and output requirements, ensuring that they conform to the definitions of audio classification.\n\n", "value": 0.6666666661666666}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [10, 15], [10, 16], [10, 17], [7, 18], [7, 19], [7, 20], [5, 21], [5, 22], [5, 23], [11, 24], [11, 25], [11, 26], [8, 27], [8, 28], [8, 29], [6, 30], [6, 31], [6, 32], [12, 33], [12, 34], [12, 35], [9, 36], [9, 37], [9, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 10], [5, 7], [6, 5], [7, 11], [8, 8], [9, 6], [10, 12], [11, 9], [12, 13], [18, 16], [21, 14], [30, 15]], {"0": {"ucb": 1.5618372455823475, "p_ucb": 1.4656535209791128, "var_p_ucb": 1.6884769395327779, "action": "Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\n", "prob": 0.9}, "1": {"ucb": 1.7221434532544055, "p_ucb": 1.5538219351987448, "var_p_ucb": 1.799340331568061, "action": "Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\n", "prob": 0.85}, "2": {"ucb": 1.7221434532544055, "p_ucb": 1.4977147625135245, "var_p_ucb": 1.7287909002728807, "action": "Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\n", "prob": 0.8}, "3": {"ucb": 2.4599451117892777, "p_ucb": 2.27395060056535, "var_p_ucb": 2.519932060810777, "action": "Clue of Step 2:Identify the components of the user request to generate a conversational response appropriate to the context of benefits associated with regular exercise.\n", "prob": 0.9}, "4": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Break down the conversational generation task into a structured outline, considering relevant areas related to exercise benefits like physical, mental, and social advantages.\n", "prob": 0.8}, "5": {"ucb": 2.4599451117892777, "p_ucb": 2.180953344953386, "var_p_ucb": 2.4132691685185117, "action": "Clue of Step 2:Ensure to incorporate user-friendly language and coherent structure in the conversational response to enhance readability and engagement.\n", "prob": 0.85}, "6": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Identify the specific conversational response model that can generate text based on the given question.\n", "prob": 0.9}, "7": {"ucb": 3.277132397641701, "p_ucb": 2.741705918023361, "var_p_ucb": 3.0258183344756153, "action": "Clue of Step 2:Ensure that the model can understand the context of the question to provide a relevant response.\n", "prob": 0.8}, "8": {"ucb": 3.277132397641701, "p_ucb": 2.8755625379279457, "var_p_ucb": 3.177431980408466, "action": "Clue of Step 2:Plan to invoke the conversational response generation tool with the user's question as input.\n", "prob": 0.85}, "9": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Break down the user request into generating a conversational response based on the input question.\n", "prob": 0.9}, "10": {"ucb": 3.277132397641701, "p_ucb": 2.741705918023361, "var_p_ucb": 3.0258183344756153, "action": "Clue of Step 2:Identify any underlying information or context needed to formulate the response effectively.\n", "prob": 0.8}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Ensure the conversational response is coherent and aligned with the user's question by reviewing the main points related to the benefits of exercising regularly.\n", "prob": 0.85}, "12": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Determine the information and knowledge needed to generate a coherent and relevant conversational response about the benefits of exercising regularly.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Research or utilize existing frameworks to formulate a conversational response that is both engaging and informative related to exercise benefits.\n", "prob": 0.85}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Review generated response to ensure clarity, relevance, and adherence to conversational norms, making adjustments as needed.\n", "prob": 0.8}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "16": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "17": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "18": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Generate a conversational response that directly answers the user's question about the benefits of exercising regularly.\n", "prob": 0.9}, "19": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure the response includes several benefits of exercising, presented in a clear and conversational tone.\n", "prob": 0.8}, "20": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "Clue of Step 3:Introduce relevant examples or analogies in the response to enhance the user's understanding and engagement.\n", "prob": 0.7}, "21": {"ucb": 2.69629414748641, "p_ucb": 2.69629414748641, "var_p_ucb": 2.8726303426294413, "action": "\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "30": {"ucb": 2.69629414748641, "p_ucb": 2.69629414748641, "var_p_ucb": 2.8726303426294413, "action": "\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:To generate a conversational response, aggregate knowledge about the benefits of exercising regularly from various credible sources.\n", "prob": 0.9}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Identify key points about the benefits of exercising regularly, such as health improvements, mental well-being, and social interactions.\n", "prob": 0.85}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Formulate the information into a coherent, engaging conversational response that addresses the user's question directly.\n", "prob": 0.9}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:For the conversational response, ensure it provides a clear, concise summary of the benefits of exercising regularly, addressing physical, mental, and social aspects.\n", "prob": 0.9}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Incorporate supportive examples or statistical data to enhance the credibility of the response regarding the benefits of regular exercise.\n", "prob": 0.8}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Frame the answer in a friendly and engaging manner to match conversational tone, making it appealing to the user.\n", "prob": 0.85}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Use a conversational response generation model to create a response about the benefits of exercising regularly, ensuring it spans various aspects such as physical health, mental well-being, and social impacts.\n", "prob": 0.9}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the response produced is coherent, informative, and relevant to the question, addressing the user's request for benefits in a comprehensive manner.\n", "prob": 0.85}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Review the generated response for clarity and engagement, incorporating any additional details or examples that enhance the conversational quality.\n", "prob": 0.8}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Iterate on the generated conversational response to ensure clarity, coherence, and comprehensiveness in addressing the user's question about the benefits of exercising regularly.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Review the conversational response for engaging language and an approachable tone that would resonate well with the user.\n", "prob": 0.85}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Ensure that the response includes a balanced overview of the benefits, potentially touching on physical, mental, and emotional aspects of exercising regularly.\n", "prob": 0.8}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Establish a clear method for generating the conversational response model, ensuring it is relevant and informative based on the user's inquiry.\n", "prob": 0.9}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Validate the generated response for coherence and correctness to ensure it effectively addresses the user's question.\n", "prob": 0.8}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Consider adding contextual elements or examples to enhance the quality and depth of the conversational response.\n", "prob": 0.85}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the conversational response is coherent and aligns with the user's question about exercising regularly.\n", "prob": 0.9}, "49": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Compile relevant points about the benefits of regular exercise from trusted sources or prior knowledge to formulate the response.\n", "prob": 0.8}, "50": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Review the generated conversational response to ensure it answers the question effectively and includes sufficient detail.\n", "prob": 0.85}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\n", "value": 0.6666666661666666}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\n", "value": 0.6666666661666666}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\n", "value": 0.6666666661666666}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\nClue of Step 2:Identify the components of the user request to generate a conversational response appropriate to the context of benefits associated with regular exercise.\n", "value": 0.6666666661666666}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\nClue of Step 2:Break down the conversational generation task into a structured outline, considering relevant areas related to exercise benefits like physical, mental, and social advantages.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\nClue of Step 2:Ensure to incorporate user-friendly language and coherent structure in the conversational response to enhance readability and engagement.\n", "value": 0.6666666661666666}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\nClue of Step 2:Identify the specific conversational response model that can generate text based on the given question.\n", "value": 0.6666666661666666}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\nClue of Step 2:Ensure that the model can understand the context of the question to provide a relevant response.\n", "value": 0.6666666661666666}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\nClue of Step 2:Plan to invoke the conversational response generation tool with the user's question as input.\n", "value": 0.6666666661666666}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\nClue of Step 2:Break down the user request into generating a conversational response based on the input question.\n", "value": 0.6666666661666666}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\nClue of Step 2:Identify any underlying information or context needed to formulate the response effectively.\n", "value": 0.6666666661666666}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\nClue of Step 2:Ensure the conversational response is coherent and aligned with the user's question by reviewing the main points related to the benefits of exercising regularly.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\nClue of Step 2:Identify the components of the user request to generate a conversational response appropriate to the context of benefits associated with regular exercise.\nClue of Step 3:Determine the information and knowledge needed to generate a coherent and relevant conversational response about the benefits of exercising regularly.\n", "value": 0.6666666661666666}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:First, identify the input question and determine that the user seeks a generated conversational response relating to the benefits of exercising.\nClue of Step 2:Ensure to incorporate user-friendly language and coherent structure in the conversational response to enhance readability and engagement.\nClue of Step 3:Generate a conversational response that directly answers the user's question about the benefits of exercising regularly.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the task requires generating informative and coherent conversational text based on the user\u2019s question.\nClue of Step 2:Identify the specific conversational response model that can generate text based on the given question.\n\n", "value": 0.6666666661666666}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on decomposing the task into generating an appropriate conversational response as the primary action to address the user's request.\nClue of Step 2:Break down the user request into generating a conversational response based on the input question.\n\n", "value": 0.6666666661666666}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [4, 12], [4, 13], [4, 14], [10, 15], [10, 16], [10, 17], [7, 18], [7, 19], [7, 20], [6, 21], [6, 22], [6, 23], [9, 24], [9, 25], [9, 26], [12, 27], [12, 28], [12, 29], [5, 30], [5, 31], [5, 32], [11, 33], [11, 34], [11, 35], [8, 36], [8, 37], [8, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [15, 45], [15, 46], [15, 47], [14, 48], [14, 49], [14, 50]], [[0, 1], [1, 3], [2, 2], [3, 4], [4, 10], [5, 7], [6, 6], [7, 9], [8, 12], [9, 5], [10, 11], [11, 8], [12, 13], [18, 16], [27, 15], [30, 14]], {"0": {"ucb": 1.5618372455823475, "p_ucb": 1.4656535209791128, "var_p_ucb": 1.6884769395327779, "action": "Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\n", "prob": 0.9}, "1": {"ucb": 1.7221434532544055, "p_ucb": 1.5538219351987448, "var_p_ucb": 1.799340331568061, "action": "Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\n", "prob": 0.85}, "2": {"ucb": 1.7221434532544055, "p_ucb": 1.609929107883965, "var_p_ucb": 1.8698897628632412, "action": "Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\n", "prob": 0.9}, "3": {"ucb": 2.4599451117892777, "p_ucb": 2.27395060056535, "var_p_ucb": 2.519932060810777, "action": "Clue of Step 2:Identify the tool that converts text into an audio format based on the user request.\n", "prob": 0.9}, "4": {"ucb": 3.3899176679089167, "p_ucb": 2.8319341342371334, "var_p_ucb": 3.15990941456437, "action": "Clue of Step 2:Ensure that the input type of the identified tool matches the user input.\n", "prob": 0.8}, "5": {"ucb": 2.4599451117892777, "p_ucb": 2.180953344953386, "var_p_ucb": 2.4132691685185117, "action": "Clue of Step 2:Check if the output of the selected tool is suitable for fulfilling the user's request for an audio file.\n", "prob": 0.85}, "6": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Identify the main actions needed to fulfill the user request, which includes converting provided text into audio format.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Consider the input type required for the selected tool to ensure it aligns with the user request for audio output.\n", "prob": 0.85}, "8": {"ucb": 2.3847549316111336, "p_ucb": 2.027803945198907, "var_p_ucb": 2.2172122228337434, "action": "Clue of Step 2:Establish the connection between the generated text and the resulting audio file based on the proposed task structure.\n", "prob": 0.8}, "9": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Identify the necessary sub-tasks involved in converting text to audio, which include text processing and speech synthesis.\n", "prob": 0.9}, "10": {"ucb": 3.277132397641701, "p_ucb": 2.741705918023361, "var_p_ucb": 3.0258183344756153, "action": "Clue of Step 2:Establish the order of operations, ensuring that text processing must occur before speech synthesis can be executed.\n", "prob": 0.8}, "11": {"ucb": 3.277132397641701, "p_ucb": 2.8755625379279457, "var_p_ucb": 3.177431980408466, "action": "Clue of Step 2:Determine the appropriate tools from the provided task list necessary to achieve each sub-task, focusing on input and output requirements.\n", "prob": 0.85}, "12": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Identify the sub-task of converting text into audio using the appropriate tool from the task list.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the selected tool matches the input and output specifications required for the conversion.\n", "prob": 0.85}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Visualize the final output as an audio file which corresponds with the intended user request.\n", "prob": 0.8}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific AI tool needed for converting text into audio, which in this case is Text-to-Speech.\n", "prob": 0.9}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Determine that the input type for the Text-to-Speech tool matches the user input, which is text.\n", "prob": 0.8}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that the output of the Text-to-Speech task will fulfill the user request of generating an audio file from the given text.\n", "prob": 0.85}, "18": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Utilize the Text-to-Speech task to produce the audio output based on the provided text input.\n", "prob": 0.9}, "19": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the input for the Text-to-Speech task matches the required format, which should be a piece of text.\n", "prob": 0.85}, "20": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Confirm that the output from the Text-to-Speech task is structured as an audio file, fulfilling the user's request completely.\n", "prob": 0.9}, "21": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Select the appropriate tool for converting the text into audio by analyzing the requirements of the user request.\n", "prob": 0.9}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that the text input format is correctly specified to match the requirements of the selected tool for audio generation.\n", "prob": 0.8}, "23": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "Clue of Step 3:Combine the generated audio with any additional attributes needed to produce a complete audio file as output.\n", "prob": 0.7}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Select the appropriate tool for audio generation, ensuring it corresponds to the input type specified in the user request.\n", "prob": 0.9}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that the tool selected can output audio in the desired format, satisfying the user request requirements.\n", "prob": 0.8}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Plan to collect the output from the audio generation step, making it ready for any further processing if needed.\n", "prob": 0.85}, "27": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Determine the specific tool required for text-to-audio conversion, in this case, 'Text-to-Speech'.\n", "prob": 0.9}, "28": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the arguments for the 'Text-to-Speech' tool are properly formatted, referencing the input text directly.\n", "prob": 0.85}, "29": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Structurally outline the task nodes ensuring that the 'Text-to-Speech' process is directly linked to the input text.\n", "prob": 0.8}, "30": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Determine the specific processing needed to transform the input text into an appropriate format for the TTS model, ensuring the syntax and semantics are conducive to natural speech.\n", "prob": 0.9}, "31": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Identify suitable models or tools for generating audio from text, focusing on those that produce high-quality and natural-sounding speech.\n", "prob": 0.8}, "32": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure the final audio output is evaluated for clarity, intelligibility, and adherence to the original text content to confirm that the conversion is successful.\n", "prob": 0.85}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the necessary steps required to convert the text into an audio format, focusing on the Text-to-Speech task.\n", "prob": 0.9}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Evaluate the input text to ensure it is appropriate for text-to-speech conversion.\n", "prob": 0.8}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Define the output requirements for the audio file, including format and quality needed for the user.\n", "prob": 0.85}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the necessary transformations and functions that need to be called to convert text to audio.\n", "prob": 0.9}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that the selected task, 'Text to Speech', captures all elements of the input text to accurately generate the audio output.\n", "prob": 0.85}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Establish a clear order of execution for the task to ensure that text is processed correctly and the audio is generated seamlessly.\n", "prob": 0.8}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Verify that the selected tool for audio generation matches the input text and produces the expected output format.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the audio file output aligns with user expectations by checking its quality and clarity after generation.\n", "prob": 0.85}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Confirm that all necessary parameters for the Text-to-Speech task are set properly to optimize the speech output.\n", "prob": 0.8}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure all necessary components for converting the text into audio are addressed, including input text and expected output format.\n", "prob": 0.9}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Verify that the selected task, 'Text-to-Speech', is capable of producing audio files and aligns with the input type of 'text'.\n", "prob": 0.85}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Confirm that the generated audio correctly represents the original text while considering factors such as pronunciation and naturalness of the speech.\n", "prob": 0.8}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the output from the Text-to-Speech model is saved as an audio file format (e.g., MP3, WAV).\n", "prob": 0.9}, "49": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Verify the audio output to confirm the speech quality and clarity match the original text.\n", "prob": 0.85}, "50": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Document the entire process, noting any potential issues or adjustments made during the text-to-audio conversion.\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\n", "value": 0.6666666661666666}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\n", "value": 0.6666666661666666}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\n", "value": 0.6666666661666666}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\nClue of Step 2:Identify the tool that converts text into an audio format based on the user request.\n", "value": 0.6666666661666666}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\nClue of Step 2:Ensure that the input type of the identified tool matches the user input.\n", "value": 0.6666666661666666}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\nClue of Step 2:Check if the output of the selected tool is suitable for fulfilling the user's request for an audio file.\n", "value": 0.6666666661666666}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\nClue of Step 2:Identify the main actions needed to fulfill the user request, which includes converting provided text into audio format.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\nClue of Step 2:Consider the input type required for the selected tool to ensure it aligns with the user request for audio output.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\nClue of Step 2:Establish the connection between the generated text and the resulting audio file based on the proposed task structure.\n", "value": 0.6666666661666666}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\nClue of Step 2:Identify the necessary sub-tasks involved in converting text to audio, which include text processing and speech synthesis.\n", "value": 0.6666666661666666}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\nClue of Step 2:Establish the order of operations, ensuring that text processing must occur before speech synthesis can be executed.\n", "value": 0.6666666661666666}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\nClue of Step 2:Determine the appropriate tools from the provided task list necessary to achieve each sub-task, focusing on input and output requirements.\n", "value": 0.6666666661666666}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\nClue of Step 2:Identify the tool that converts text into an audio format based on the user request.\nClue of Step 3:Identify the sub-task of converting text into audio using the appropriate tool from the task list.\n", "value": 0.6666666661666666}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the primary action required from the user request, which is to convert text to audio.\nClue of Step 2:Check if the output of the selected tool is suitable for fulfilling the user's request for an audio file.\nClue of Step 3:Utilize the Text-to-Speech task to produce the audio output based on the provided text input.\n", "value": 0.6666666661666666}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct actions: input text gathering and audio file generation.\nClue of Step 2:Establish the connection between the generated text and the resulting audio file based on the proposed task structure.\nClue of Step 3:Determine the specific tool required for text-to-audio conversion, in this case, 'Text-to-Speech'.\n", "value": 0.6666666661666666}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on the specific requirements of the task by understanding the input type (text) and the desired output type (audio).\nClue of Step 2:Identify the necessary sub-tasks involved in converting text to audio, which include text processing and speech synthesis.\nClue of Step 3:Determine the specific processing needed to transform the input text into an appropriate format for the TTS model, ensuring the syntax and semantics are conducive to natural speech.\n", "value": 0.6666666661666666}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [6, 19], [9, 20], [9, 21], [9, 22], [12, 23], [12, 24], [12, 25], [8, 26], [8, 27], [8, 28], [11, 29], [11, 30], [11, 31], [5, 32], [5, 33], [5, 34], [13, 35], [13, 36], [13, 37], [16, 38], [16, 39], [16, 40], [15, 41], [15, 42], [15, 43], [14, 44], [14, 45], [14, 46]], [[0, 1], [1, 3], [2, 2], [3, 4], [4, 7], [5, 10], [6, 6], [7, 9], [8, 12], [9, 8], [10, 11], [11, 5], [12, 13], [17, 16], [19, 15], [33, 14]], {"0": {"ucb": 0.9618372460323477, "p_ucb": 0.865653521429113, "var_p_ucb": 1.088476939982778, "action": "Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\n", "prob": 0.9}, "1": {"ucb": 1.1221434537044057, "p_ucb": 0.8977147629635246, "var_p_ucb": 1.128790900722881, "action": "Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\n", "prob": 0.8}, "2": {"ucb": 1.1221434537044057, "p_ucb": 0.9538219356487447, "var_p_ucb": 1.1993403320180611, "action": "Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\n", "prob": 0.85}, "3": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "Clue of Step 2:List the various subtasks identified in Step 1 and determine their execution order based on dependencies.\n", "prob": 0.9}, "4": {"ucb": 1.8599451122392778, "p_ucb": 1.580953345403386, "var_p_ucb": 1.8132691689685119, "action": "Clue of Step 2:Identify the input requirements for each subtask from the available tools in the task list.\n", "prob": 0.85}, "5": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Create connections among subtasks to reflect input-output dependencies, ensuring that each step logically follows from the last.\n", "prob": 0.8}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Define the main actions needed to create a summarization from the document image and enhance the speech quality for narration.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Establish a sequence to first summarize the content of the document from the image, then generate text-to-speech for the summarization.\n", "prob": 0.85}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Identify the tools required for each action: use Image-to-Text for extracting information from the document, and Text-to-Speech for generating audio narration based on the summary.\n", "prob": 0.8}, "9": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Break down the user request into two main components: creating a video summarizing the document image and enhancing the speech quality of the narration.\n", "prob": 0.9}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Identify the necessary tools from the task list that can perform the summarization and speech enhancement tasks.\n", "prob": 0.85}, "11": {"ucb": 1.7847549320611338, "p_ucb": 1.695517185458077, "var_p_ucb": 1.9204395151494456, "action": "Clue of Step 2:Ensure that the steps identified are logically sequenced based on the dependencies, starting with summarization before enhancing the narration.\n", "prob": 0.95}, "12": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "15": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Identify the specific tools needed for each sub-task, ensuring they match the input types required.\n", "prob": 0.9}, "16": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the sequence of tools reflects the natural flow of tasks from summarization to narration and speech enhancement.\n", "prob": 0.85}, "17": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Clue of Step 3:Confirm that all output types from previous tasks can be effectively used as input types for subsequent tasks, maintaining coherence in the workflow.\n", "prob": 0.95}, "18": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Specify the exact details of the video summarization process, including how to extract key content from the document and convert it to video format.\n", "prob": 0.9}, "19": {"ucb": 2.09629414793641, "p_ucb": 1.7818500257459486, "var_p_ucb": 1.9317357916175248, "action": "Clue of Step 3:Outline the process for document summarization, including extracting the key information from the document image and generating a coherent summary.\n", "prob": 0.85}, "20": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Determine the necessary sequence of tasks needed to create the video, which includes summarizing the document and generating audio before merging them with visuals.\n", "prob": 0.9}, "21": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 3:Ensure that the summarization task output becomes the input for the text-to-speech task, confirming that all dependencies are tracked correctly.\n", "prob": 0.95}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Once the audio and visuals are ready, specify how to integrate them into a final video product, considering the timing and quality of the components.\n", "prob": 0.85}, "23": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Establish the sequence of tasks by mapping the creation of the video and the enhancement of speech quality, ensuring that the video can only be generated after summarizing the image and audio has been prepared.\n", "prob": 0.9}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that the video summarization model incorporates the appropriate textual content derived from the image and that speech quality enhancement is applied only after generating the initial audio narration.\n", "prob": 0.8}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Create a final task structure that clearly outlines which tools will be used and their specific dependencies, verifying that the results from each task feed accurately into subsequent tasks.\n", "prob": 0.85}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Assess the types of input required for summarization and speech enhancement, identifying any need for intermediate outputs to provide the necessary input.\n", "prob": 0.9}, "30": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that the visual content and audio narration are linked such that the final output serves the user request of creating a video summarizing the document.\n", "prob": 0.85}, "31": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 3:Verify that the workflow maintains clarity in how the document's content is summarized and presented in both visual and audio formats.\n", "prob": 0.95}, "32": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Determine the sequence of sub-tasks needed to create a video summarizing the content of the document image while addressing the quality of narration.\n", "prob": 0.85}, "33": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Identify the tools required for video creation, speech enhancement, and ensure they are properly linked based on the input and output types.\n", "prob": 0.9}, "34": {"ucb": 4.19258829587282, "p_ucb": 3.689477700368082, "var_p_ucb": 3.9998294038198163, "action": "Clue of Step 3:Ensure that the dependencies between text summarization, video generation, and audio enhancement are logically followed to accomplish the user request effectively.\n", "prob": 0.88}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Verify the tasks are aligned with the desired output, ensuring all inputs and expected outputs are covered.\n", "prob": 0.9}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Check if the selected tools effectively accomplish each identified sub-task from the user request.\n", "prob": 0.85}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.930592231275096, "var_p_ucb": 3.1228129000782543, "action": "Clue of Step 4:Reassess dependencies to confirm that all tasks flow logically and no necessary steps are omitted.\n", "prob": 0.88}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the output of the summarization task can be used as input for the narration generation task.\n", "prob": 0.9}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Check if the output from the narration generation can subsequently be processed by the audio enhancement task.\n", "prob": 0.85}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Validate that each step logically contributes to achieving the final goal of creating a video, maintaining coherence of the provided information.\n", "prob": 0.9}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the task links accurately reflect the order in which tasks need to be executed, especially considering the input and output types of each task.\n", "prob": 0.85}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Verify that all necessary input types are provided for each task, and that outputs are properly interlinked as inputs for subsequent tasks.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Cross-check the dependencies identified and ensure they align with the goals set by the user request to maintain the overall coherence of the solution.\n", "prob": 0.8}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure the identified tasks are capable of producing the desired output, including a video format as specified by the user request.\n", "prob": 0.85}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Implement a clear distinction between tasks involving text processing (like summarization) and those involving audio enhancement.\n", "prob": 0.9}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Reassess the connections between tasks to guarantee that the output from one task feeds correctly into the next, especially focusing on the relationship between narration, summarization, and enhancement.\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\nClue of Step 2:List the various subtasks identified in Step 1 and determine their execution order based on dependencies.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\nClue of Step 2:Identify the input requirements for each subtask from the available tools in the task list.\n", "value": 0.0}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\nClue of Step 2:Create connections among subtasks to reflect input-output dependencies, ensuring that each step logically follows from the last.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\nClue of Step 2:Define the main actions needed to create a summarization from the document image and enhance the speech quality for narration.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\nClue of Step 2:Establish a sequence to first summarize the content of the document from the image, then generate text-to-speech for the summarization.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\nClue of Step 2:Identify the tools required for each action: use Image-to-Text for extracting information from the document, and Text-to-Speech for generating audio narration based on the summary.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\nClue of Step 2:Break down the user request into two main components: creating a video summarizing the document image and enhancing the speech quality of the narration.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\nClue of Step 2:Identify the necessary tools from the task list that can perform the summarization and speech enhancement tasks.\n", "value": 0.0}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\nClue of Step 2:Ensure that the steps identified are logically sequenced based on the dependencies, starting with summarization before enhancing the narration.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\nClue of Step 2:List the various subtasks identified in Step 1 and determine their execution order based on dependencies.\n\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the user request into specific tasks: summarizing the document, generating narration, and enhancing speech quality.\nClue of Step 2:Identify the input requirements for each subtask from the available tools in the task list.\nClue of Step 3:Confirm that all output types from previous tasks can be effectively used as input types for subsequent tasks, maintaining coherence in the workflow.\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the inputs required for each task: the document image and the topic for summarization.\nClue of Step 2:Define the main actions needed to create a summarization from the document image and enhance the speech quality for narration.\nClue of Step 3:Outline the process for document summarization, including extracting the key information from the document image and generating a coherent summary.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the tasks identified cover all aspects of the user request, including visual and auditory elements.\nClue of Step 2:Ensure that the steps identified are logically sequenced based on the dependencies, starting with summarization before enhancing the narration.\nClue of Step 3:Identify the tools required for video creation, speech enhancement, and ensure they are properly linked based on the input and output types.\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [4, 12], [10, 13], [10, 14], [10, 15], [7, 16], [7, 17], [7, 18], [6, 19], [6, 20], [6, 21], [9, 22], [9, 23], [9, 24], [12, 25], [12, 26], [12, 27], [5, 28], [5, 29], [5, 30], [8, 31], [8, 32], [8, 33], [11, 34], [11, 35], [11, 36], [13, 37], [13, 38], [13, 39], [14, 40], [14, 41], [14, 42], [16, 43], [16, 44], [16, 45], [15, 46], [15, 47], [15, 48]], [[0, 1], [1, 3], [2, 2], [3, 4], [4, 10], [5, 7], [6, 6], [7, 9], [8, 12], [9, 5], [10, 8], [11, 11], [12, 13], [16, 14], [21, 16], [29, 15]], {"0": {"ucb": 1.2048372456678478, "p_ucb": 1.1086535210646131, "var_p_ucb": 1.3314769396182782, "action": "Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\n", "prob": 0.9}, "1": {"ucb": 1.1221434537044057, "p_ucb": 0.8977147629635246, "var_p_ucb": 1.128790900722881, "action": "Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\n", "prob": 0.8}, "2": {"ucb": 1.1221434537044057, "p_ucb": 0.9538219356487447, "var_p_ucb": 1.1993403320180611, "action": "Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\n", "prob": 0.85}, "3": {"ucb": 2.129945111834278, "p_ucb": 1.94395060061035, "var_p_ucb": 2.1899320608557775, "action": "Clue of Step 2:Segment the input image into different objects using Image Segmentation, which will allow for precise identification before labeling.\n", "prob": 0.9}, "4": {"ucb": 2.7899176683589166, "p_ucb": 2.371430018105079, "var_p_ucb": 2.7199037534527677, "action": "Clue of Step 2:Label the segmented objects in the image using Object Detection to differentiate between various objects and provide appropriate labels.\n", "prob": 0.85}, "5": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "Clue of Step 2:Utilize Document Question Answering to answer the provided question based on the identified objects and the given text context, ensuring the response is relevant to the segmented objects.\n", "prob": 0.9}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Identify the tasks that correspond to the actions needed for segmenting objects and answering questions based on the input image and text.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Determine the dependencies between the sub-tasks, specifically which task's output is needed as input for another task.\n", "prob": 0.85}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Use a structured format to outline the sequence of tasks that need to be executed to fulfill the user request, ensuring clarity and coherence.\n", "prob": 0.8}, "9": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Break the user request into specific sub-tasks: segment objects, label them, and conduct question answering based on provided text.\n", "prob": 0.9}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Identify task dependencies, noting that segmenting must occur before labeling, and that both should be completed before answering the question.\n", "prob": 0.85}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Ensure that the chosen tools for each sub-task can properly handle their respective inputs and outputs as specified in the task list.\n", "prob": 0.8}, "12": {"ucb": 2.39629414748641, "p_ucb": 2.186664732692769, "var_p_ucb": 2.3453673083214968, "action": "Clue of Step 3:Perform Image Segmentation on the input image to delineate individual objects, which will provide the basis for further labeling and analysis.\n", "prob": 0.9}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "16": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Utilize Image Segmentation to identify and separate different objects in the provided image.\n", "prob": 0.9}, "17": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "Clue of Step 3:Apply Image-to-Text model to generate text descriptions for the segmented objects, if required.\n", "prob": 0.7}, "18": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Use Document Question Answering to respond to the question based on the segmented objects and the provided text context.\n", "prob": 0.85}, "19": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Determine the specific tools needed to segment and label objects in the image.\n", "prob": 0.9}, "20": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure the chosen tools match the required input types for each sub-task: image for segmentation and text for answering questions.\n", "prob": 0.85}, "21": {"ucb": 2.09629414793641, "p_ucb": 1.9285906161014974, "var_p_ucb": 2.090819915633086, "action": "Clue of Step 3:Plan the order of execution for tasks to first segment the image, then use the results to answer the given question.\n", "prob": 0.92}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific tasks needed to segment the objects in the image and label them, ensuring to connect these to the goal of answering the question.\n", "prob": 0.9}, "23": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Establish the input and output requirements for each sub-task to ensure compatibility and feasibility.\n", "prob": 0.85}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 3:Prioritize the sequence of sub-tasks based on their logical progression from object segmentation to answering the question based on identified objects.\n", "prob": 0.95}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Segment the objects in the image using Image Segmentation so that individual objects can be uniquely identified.\n", "prob": 0.9}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Label the segmented objects using Object Detection which will provide bounding boxes and class labels for each identified object.\n", "prob": 0.85}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.0638009690603276, "var_p_ucb": 3.2647589409909026, "action": "Clue of Step 3:Utilize Document Question Answering to answer the provided question based on both the segmented image and the associated text.\n", "prob": 0.92}, "28": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Evaluate each identified sub-task for its purpose and determine if they directly contribute to fulfilling the user request.\n", "prob": 0.85}, "29": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Consider the order of operations for the sub-tasks, ensuring that segmentation is completed before labeling, and that the question answering uses the results of the labeling.\n", "prob": 0.9}, "30": {"ucb": 4.19258829587282, "p_ucb": 3.689477700368082, "var_p_ucb": 3.9998294038198163, "action": "Clue of Step 3:Ensure that the inputs and outputs of each sub-task align correctly with the defined task list to facilitate smooth execution of the workflow.\n", "prob": 0.88}, "31": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Determine the specific tools and tasks that will resolve the user request by mapping each identified sub-task to the appropriate tool configurations.\n", "prob": 0.85}, "32": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Establish a clear sequence for executing the sub-tasks based on their input-output dependencies, ensuring that output from one task serves as the input for another where necessary.\n", "prob": 0.9}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Review the task nodes to ensure that all required inputs and outputs align with the expected data formats, thus confirming that the sequence will function correctly.\n", "prob": 0.8}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific tools required for each sub-task based on the input and output types, ensuring that each tool aligns with the functional requirements of the user request.\n", "prob": 0.9}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Sequentially arrange the selected tools according to their processing order to maintain dependency flow, ensuring that each output is suitable as input for subsequent tasks.\n", "prob": 0.8}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Verify the compatibility of the tools' input and output types with the respective data being processed at each step to ensure accuracy and effectiveness in achieving the overall goal.\n", "prob": 0.85}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Use Document Question Answering to provide a detailed answer to the question based on the segment labels from the previous tasks.\n", "prob": 0.85}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Establish connections between object labels obtained from segmentation and the provided text to identify the most commonly found object.\n", "prob": 0.9}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.930592231275096, "var_p_ucb": 3.1228129000782543, "action": "Clue of Step 4:Combine the results from the Image Segmentation and Document Question Answering to formulate a cohesive answer that addresses the user\u2019s question.\n", "prob": 0.88}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Incorporate the segments from the Image Segmentation step to feed into the Document Question Answering model for answering the user's question accurately.\n", "prob": 0.9}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the output from the Image Segmentation task is appropriately formatted and labeled so that it can effectively inform the Document Question Answering process.\n", "prob": 0.85}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 4:Review the entire task plan to confirm that all elements interact seamlessly and that any outputs can be linked as inputs for the subsequent tasks.\n", "prob": 0.95}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Assess the sub-tasks to ensure they collectively cover all aspects of the user request, such as object segmentation and question answering.\n", "prob": 0.9}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Verify that chosen tasks align properly with inputs and outputs defined in the task list to facilitate a smooth process flow.\n", "prob": 0.85}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Check the dependencies once more to confirm that tasks are sequenced correctly to allow for accurate data flow between them.\n", "prob": 0.9}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Based on the identified sub-tasks, ensure that the task nodes for segmentation, labeling, and answering questions are accurately represented and aligned with the tool capabilities.\n", "prob": 0.9}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the outputs of the segmentation and labeling tasks serve as clear inputs for the question answering task to create a coherent workflow.\n", "prob": 0.85}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 4:Review the dependencies between the tasks to confirm that the execution order will lead to successful results for the user's request, ensuring each task feeds into the subsequent one.\n", "prob": 0.95}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\nClue of Step 2:Segment the input image into different objects using Image Segmentation, which will allow for precise identification before labeling.\n", "value": 0.0}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\nClue of Step 2:Label the segmented objects in the image using Object Detection to differentiate between various objects and provide appropriate labels.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\nClue of Step 2:Utilize Document Question Answering to answer the provided question based on the identified objects and the given text context, ensuring the response is relevant to the segmented objects.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\nClue of Step 2:Identify the tasks that correspond to the actions needed for segmenting objects and answering questions based on the input image and text.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\nClue of Step 2:Determine the dependencies between the sub-tasks, specifically which task's output is needed as input for another task.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\nClue of Step 2:Use a structured format to outline the sequence of tasks that need to be executed to fulfill the user request, ensuring clarity and coherence.\n", "value": 0.0}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\nClue of Step 2:Break the user request into specific sub-tasks: segment objects, label them, and conduct question answering based on provided text.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\nClue of Step 2:Identify task dependencies, noting that segmenting must occur before labeling, and that both should be completed before answering the question.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\nClue of Step 2:Ensure that the chosen tools for each sub-task can properly handle their respective inputs and outputs as specified in the task list.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\nClue of Step 2:Segment the input image into different objects using Image Segmentation, which will allow for precise identification before labeling.\nClue of Step 3:Perform Image Segmentation on the input image to delineate individual objects, which will provide the basis for further labeling and analysis.\n", "value": 0.33333333283333333}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key components of the user request, such as the need for image segmentation, labeling, and answering a specific question based on given text.\nClue of Step 2:Utilize Document Question Answering to answer the provided question based on the identified objects and the given text context, ensuring the response is relevant to the segmented objects.\nClue of Step 3:Utilize Image Segmentation to identify and separate different objects in the provided image.\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the task of segmenting the objects in the image from the task of answering questions, as these are distinct but interrelated tasks.\nClue of Step 2:Identify the tasks that correspond to the actions needed for segmenting objects and answering questions based on the input image and text.\nClue of Step 3:Plan the order of execution for tasks to first segment the image, then use the results to answer the given question.\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the identified sub-tasks align with the capabilities of available tools for processing images and generating text responses.\nClue of Step 2:Break the user request into specific sub-tasks: segment objects, label them, and conduct question answering based on provided text.\nClue of Step 3:Consider the order of operations for the sub-tasks, ensuring that segmentation is completed before labeling, and that the question answering uses the results of the labeling.\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [5, 12], [5, 13], [5, 14], [8, 15], [8, 16], [8, 17], [11, 18], [11, 19], [11, 20], [9, 21], [9, 22], [9, 23], [6, 24], [6, 25], [6, 26], [12, 27], [12, 28], [12, 29], [4, 30], [4, 31], [4, 32], [10, 33], [10, 34], [10, 35], [7, 36], [7, 37], [7, 38], [14, 39], [15, 40], [15, 41], [15, 42], [13, 43], [13, 44], [13, 45], [16, 46], [16, 47], [16, 48]], [[0, 1], [1, 3], [2, 2], [3, 5], [4, 8], [5, 11], [6, 9], [7, 6], [8, 12], [9, 4], [10, 10], [11, 7], [14, 14], [24, 15], [30, 13], [36, 16]], {"0": {"ucb": 1.1221434537044057, "p_ucb": 0.9538219356487447, "var_p_ucb": 1.1993403320180611, "action": "Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\n", "prob": 0.85}, "1": {"ucb": 1.1221434537044057, "p_ucb": 0.8977147629635246, "var_p_ucb": 1.128790900722881, "action": "Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\n", "prob": 0.8}, "2": {"ucb": 0.9618372460323477, "p_ucb": 0.865653521429113, "var_p_ucb": 1.088476939982778, "action": "Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\n", "prob": 0.9}, "3": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Identify the sequential steps required to achieve the user goal, starting from audio enhancement to modifying an image and answering questions.\n", "prob": 0.9}, "4": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Map out the inputs and outputs of each selected task to ensure they align correctly with the required operations.\n", "prob": 0.85}, "5": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Prioritize the tasks based on the flow of information\u2014transcribe audio before modifying the image and asking a question about it.\n", "prob": 0.8}, "6": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Identify the relationships among the sound enhancement, source separation, transcription, image modification, depth estimation, and question answering sub-tasks.\n", "prob": 0.85}, "7": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Sequence the identified tasks ensuring that the output of one task serves as the input to the next appropriately.\n", "prob": 0.9}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Outline the specific tools required for each sub-task and confirm they align with the inputs and outputs defined for these tasks.\n", "prob": 0.8}, "9": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "Clue of Step 2:Break down the complex user request into distinct sub-tasks such as audio enhancement and separation, transcription, image modification, depth estimation, and question answering.\n", "prob": 0.9}, "10": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Identify the input types needed for each sub-task; for example, specify which sub-tasks require audio input, image input, and text input.\n", "prob": 0.8}, "11": {"ucb": 1.8599451122392778, "p_ucb": 1.580953345403386, "var_p_ucb": 1.8132691689685119, "action": "Clue of Step 2:Establish the order of execution for the sub-tasks based on their dependencies, ensuring that outputs from some tasks serve as inputs for subsequent tasks.\n", "prob": 0.85}, "12": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Determine the order of sub-tasks needed to enhance audio, transcribe it to text, modify the image, and answer the question sequentially.\n", "prob": 0.85}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that any models used for processing are compatible with the inputs expected for each sub-task, such as the type of audio or image data.\n", "prob": 0.8}, "14": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Check that the final step of generating a video based on the answered question logically follows from the previous outputs.\n", "prob": 0.9}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the sequence of tasks: first enhance and separate the audio, then transcribe it, use the text for image modification, estimate depth, and finally answer the question and generate a video.\n", "prob": 0.9}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure all inputs and outputs of each task are appropriately linked to maintain the flow and integrity of the overall process.\n", "prob": 0.85}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.930592231275096, "var_p_ucb": 3.1228129000782543, "action": "Clue of Step 3:Create a detailed task plan by confirming that each sub-task effectively contributes to achieving the final goal of generating a video based on the user\u2019s request.\n", "prob": 0.88}, "18": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Ensure that the audio enhancement and source separation tasks are completed before transcription, as the output of these tasks will serve as the input for transcription.\n", "prob": 0.9}, "19": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Link the transcription output to the subsequent image modification task, ensuring that the modified image follows the completion of transcription.\n", "prob": 0.85}, "20": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Conclude the process by linking the image modification output to depth estimation and then correlate this with the question-answering task, making sure all preceding tasks are completed in sequence.\n", "prob": 0.9}, "21": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Map each sub-task to a task node from the provided task list, ensuring all input-types align.\n", "prob": 0.9}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Establish a clear sequence for executing each task node, taking into account the dependencies derived from the user request.\n", "prob": 0.85}, "23": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 3:Verify that all outputs from one task node correctly serve as inputs for the subsequent tasks, ensuring the flow of information is maintained.\n", "prob": 0.95}, "24": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Create a sequence of tasks that begins with enhancing the audio and progresses through transcription, image modification, depth estimation, visual questioning, and video generation.\n", "prob": 0.9}, "25": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that the audio enhancement is processed first, as the text generated from it is essential for subsequent tasks involving image modification.\n", "prob": 0.8}, "26": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Link each task clearly by identifying and applying the outputs from one task as inputs to the next to maintain the flow and coherence of the overall process.\n", "prob": 0.85}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Organize the identified sub-tasks by linking them according to their input-output relationships, ensuring that each step feeds correctly into the next.\n", "prob": 0.85}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Create a structured flow from audio enhancement to transcription, ensuring the output of each task is used as the input for the next relevant task.\n", "prob": 0.9}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Verify the dependencies and ensure that the output of each sub-task can facilitate the next required task before finalizing the task plan.\n", "prob": 0.8}, "30": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Identify and select specific tasks from the provided task list that align with the necessary operations of audio enhancement, transcription, image modification, depth estimation, and question answering.\n", "prob": 0.9}, "31": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure the selected tasks have clearly defined input and output types that correspond to the requirements of the user request.\n", "prob": 0.85}, "32": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Establish a logical flow among the selected tasks by defining their interdependencies, ensuring that outputs of prior tasks can serve as inputs for subsequent ones.\n", "prob": 0.9}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Create a sequence of tasks that flow logically, beginning with audio enhancement and separation, followed by transcription of the enhanced audio, and culminating in image modification.\n", "prob": 0.9}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that all input and output types are respected between tasks, particularly ensuring that the output of one task serves as the appropriate input for the next.\n", "prob": 0.85}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 3:Verify that all tasks contribute to the final goals of depth estimation and answering a question, while adhering to the defined dependencies and output requirements.\n", "prob": 0.95}, "36": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Break down the main request into distinct sub-tasks: enhancing audio, transcribing audio, modifying the image, estimating depth, and answering a question.\n", "prob": 0.9}, "37": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Identify the output requirements for each sub-task to ensure they align with the desired goals of depth estimation and responding to the question.\n", "prob": 0.8}, "38": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Map out the dependencies to ensure that the order of sub-tasks logically follows the flow of information, leading from audio enhancement to video generation.\n", "prob": 0.85}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 4:Ensure the sub-tasks for audio enhancement, transcription, image modification, depth estimation, and question answering are adequately defined to fulfill the user request.\n", "prob": 0.95}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that each specific task corresponds to the input and output types of the corresponding tool in the task list.\n", "prob": 0.85}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Check that the outputs of each task logically feed into the inputs of the next task in the sequence.\n", "prob": 0.9}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.497663833473093, "var_p_ucb": 2.6614882671121487, "action": "Clue of Step 4:Validate that all components of the request are addressed through the selected tasks and that the entire process leads to a coherent final output.\n", "prob": 0.75}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Consolidate the identified sub-tasks into a cohesive plan by specifying the tool for each sub-task and clearly mapping input-output relationships.\n", "prob": 0.85}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that each sub-task's output appropriately feeds into the next sub-task, maintaining a logical flow from audio enhancement to final video generation.\n", "prob": 0.9}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 4:Review the selected tasks and their dependencies to confirm all necessary operations (audio processing, transcription, image modifications, depth estimation, and answering questions) are included and ordered correctly.\n", "prob": 0.95}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Identify the outputs necessary for the final goals, ensuring they can be derived from the results of previous tasks.\n", "prob": 0.85}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Verify that each sub-task enhances the overall outcome effectively while aligning with the user\u2019s request components.\n", "prob": 0.8}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Check that all task nodes selected correspond to tasks in the provided task list and have clear input-output structures.\n", "prob": 0.9}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\n", "value": 0.0}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\nClue of Step 2:Identify the sequential steps required to achieve the user goal, starting from audio enhancement to modifying an image and answering questions.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\nClue of Step 2:Map out the inputs and outputs of each selected task to ensure they align correctly with the required operations.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\nClue of Step 2:Prioritize the tasks based on the flow of information\u2014transcribe audio before modifying the image and asking a question about it.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\nClue of Step 2:Identify the relationships among the sound enhancement, source separation, transcription, image modification, depth estimation, and question answering sub-tasks.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\nClue of Step 2:Sequence the identified tasks ensuring that the output of one task serves as the input to the next appropriately.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\nClue of Step 2:Outline the specific tools required for each sub-task and confirm they align with the inputs and outputs defined for these tasks.\n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\nClue of Step 2:Break down the complex user request into distinct sub-tasks such as audio enhancement and separation, transcription, image modification, depth estimation, and question answering.\n", "value": 0.0}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\nClue of Step 2:Identify the input types needed for each sub-task; for example, specify which sub-tasks require audio input, image input, and text input.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\nClue of Step 2:Establish the order of execution for the sub-tasks based on their dependencies, ensuring that outputs from some tasks serve as inputs for subsequent tasks.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the specific actions needed to process the noisy audio, such as enhancing the audio quality and separating audio sources.\nClue of Step 2:Identify the sequential steps required to achieve the user goal, starting from audio enhancement to modifying an image and answering questions.\nClue of Step 3:Check that the final step of generating a video based on the answered question logically follows from the previous outputs.\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine how the enhanced audio will be transcribed into text and how that text can be used to modify an image.\nClue of Step 2:Sequence the identified tasks ensuring that the output of one task serves as the input to the next appropriately.\nClue of Step 3:Create a sequence of tasks that begins with enhancing the audio and progresses through transcription, image modification, depth estimation, visual questioning, and video generation.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\nClue of Step 2:Break down the complex user request into distinct sub-tasks such as audio enhancement and separation, transcription, image modification, depth estimation, and question answering.\nClue of Step 3:Identify and select specific tasks from the provided task list that align with the necessary operations of audio enhancement, transcription, image modification, depth estimation, and question answering.\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Consider the final goals of depth estimation and answering a question based on the modified image to ensure all steps support these outcomes.\nClue of Step 2:Establish the order of execution for the sub-tasks based on their dependencies, ensuring that outputs from some tasks serve as inputs for subsequent tasks.\nClue of Step 3:Break down the main request into distinct sub-tasks: enhancing audio, transcribing audio, modifying the image, estimating depth, and answering a question.\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [6, 12], [6, 13], [6, 14], [9, 15], [9, 16], [9, 17], [12, 18], [12, 19], [12, 20], [5, 21], [5, 22], [5, 23], [11, 24], [11, 25], [11, 26], [8, 27], [8, 28], [8, 29], [4, 30], [4, 31], [4, 32], [7, 33], [10, 34], [10, 35], [10, 36], [16, 37], [16, 38], [16, 39], [13, 40], [13, 41], [13, 42], [15, 43], [15, 44], [15, 45], [14, 46], [14, 47], [14, 48]], [[0, 1], [1, 3], [2, 2], [3, 6], [4, 9], [5, 12], [6, 5], [7, 11], [8, 8], [9, 4], [10, 7], [11, 10], [12, 16], [18, 13], [23, 15], [32, 14]], {"0": {"ucb": 1.5018372456273477, "p_ucb": 1.405653521024113, "var_p_ucb": 1.628476939577778, "action": "Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\n", "prob": 0.9}, "1": {"ucb": 1.7221434532544055, "p_ucb": 1.4977147625135245, "var_p_ucb": 1.7287909002728807, "action": "Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\n", "prob": 0.8}, "2": {"ucb": 1.7221434532544055, "p_ucb": 1.5538219351987448, "var_p_ucb": 1.799340331568061, "action": "Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\n", "prob": 0.85}, "3": {"ucb": 2.159945111789278, "p_ucb": 1.97395060056535, "var_p_ucb": 2.2199320608107773, "action": "Clue of Step 2:Identify the primary operation needed to convert the text content within the image to digitized text through the appropriate tool.\n", "prob": 0.9}, "4": {"ucb": 3.0899176679089164, "p_ucb": 2.671430017655079, "var_p_ucb": 3.019903753002768, "action": "Clue of Step 2:Ensure that the selected tool is capable of handling image input and producing text output, aligning with the user request.\n", "prob": 0.85}, "5": {"ucb": 2.4599451117892777, "p_ucb": 2.087956089341422, "var_p_ucb": 2.3066062762262463, "action": "Clue of Step 2:Establish the sequence of actions in a logical flow from receiving the image input to producing the text output, making sure to consider any pre-processing if necessary.\n", "prob": 0.8}, "6": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Identify the necessary tasks to convert the image 'example.jpg' to text, primarily focusing on image-to-text conversion.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Ensure that the image input is properly formatted and available for the image-to-text model before executing the conversion task.\n", "prob": 0.8}, "8": {"ucb": 3.277132397641701, "p_ucb": 2.8755625379279457, "var_p_ucb": 3.177431980408466, "action": "Clue of Step 2:Sequence the tasks with a clear identification of the output of the image-to-text model that will serve as the final result of the user request.\n", "prob": 0.85}, "9": {"ucb": 2.084754931611134, "p_ucb": 1.9062794384050206, "var_p_ucb": 2.119363750744211, "action": "Clue of Step 2:Identify the specific sub-task required to convert an image containing text into its textual representation, which aligns with the 'Image to Text' classification task.\n", "prob": 0.9}, "10": {"ucb": 2.977132397641701, "p_ucb": 2.5755625379279454, "var_p_ucb": 2.877431980408466, "action": "Clue of Step 2:Determine the input required for the 'Image to Text' task, which is an image file, and confirm that the user has provided the necessary input ('example.jpg').\n", "prob": 0.85}, "11": {"ucb": 2.977132397641701, "p_ucb": 2.441705918023361, "var_p_ucb": 2.7258183344756155, "action": "Clue of Step 2:Establish that the output of the 'Image to Text' task serves the ultimate goal of converting the image into text, confirming that this output will be used for any further analysis or tasks, if required.\n", "prob": 0.8}, "12": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Use Image to Text to extract text from the uploaded image file, as it is the primary task needed to achieve the user request.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure the output from the Image to Text task is properly formatted so it can be utilized without further modification.\n", "prob": 0.85}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Verify that all necessary input types and formats are respected in the Image to Text task to ensure accurate output.\n", "prob": 0.8}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Ensure that the selected tool for the task (Image to Text) can accurately recognize and convert the text within the image into a readable format.\n", "prob": 0.9}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Verify that the tool's output can be processed as intended, confirming it aligns with the user's request to convert image content to text.\n", "prob": 0.85}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Assess any potential errors or limitations in extraction from images to ensure the accuracy and completeness of the recognized text.\n", "prob": 0.8}, "18": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "\n", "prob": 1.0}, "19": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "20": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "21": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Identify the specific task needed to convert the image 'example.jpg' containing text into a machine-readable format, which is the Image-to-Text task.\n", "prob": 0.9}, "22": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the Image-to-Text task is the only required step to produce the output as this task directly correlates with the user request to convert the image into text.\n", "prob": 0.85}, "23": {"ucb": 2.69629414748641, "p_ucb": 2.5914794400895893, "var_p_ucb": 2.758998825475469, "action": "Clue of Step 3:Verify that the task and its arguments align with the requirements defined in the task list, particularly for handling image inputs.\n", "prob": 0.95}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:After identifying the objects in the image using Object Detection, determine how to phrase the question and retrieve the necessary information from the visual context for Visual Question Answering.\n", "prob": 0.8}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Ensure that the results from both Object Detection and Visual Question Answering are accessible for comparison, as Sentence Similarity needs both outputs to function correctly.\n", "prob": 0.9}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Conclude the task by merging the outputs from Object Detection and Visual Question Answering to complete the Sentence Similarity analysis, maintaining clarity in results interpretation.\n", "prob": 0.85}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Use Image-to-Text to extract text from the provided image, ensuring you have the most relevant textual representation.\n", "prob": 0.9}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Confirm that the output from the Image-to-Text task is suitable for further processing or directly addresses the user request.\n", "prob": 0.85}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that all generated outputs are consolidated and appropriately formatted to meet the user's expectations.\n", "prob": 0.8}, "30": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Leverage the 'Image to Text' task to extract the text content from the image effectively.\n", "prob": 0.9}, "31": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure that the input for the 'Image to Text' tool is the correct image file to avoid processing errors.\n", "prob": 0.85}, "32": {"ucb": 2.39629414748641, "p_ucb": 2.2914794400895895, "var_p_ucb": 2.4589988254754687, "action": "Clue of Step 3:Plan to return the extracted text and make sure it meets the user's requirement of converting an image to text accurately.\n", "prob": 0.95}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Identify the specific input and output types required for the 'Image to Text' task, ensuring that an image file is correctly processed to generate the relevant text output.\n", "prob": 0.8}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific task in the user request which in this case is to extract text from an image, guiding you to select the necessary tool which is 'Image-to-Text'.\n", "prob": 0.9}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure that the identified task of 'Image-to-Text' aligns with the input type requirements of the tool, confirming it can process the given image filename.\n", "prob": 0.85}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Plan for execution by structuring the selected task into a sequence that follows a natural flow from input (image) to output (text), ensuring smooth transitions between tasks.\n", "prob": 0.9}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the chosen sub-tasks clearly address the user's request of converting an image to text and that they collectively fulfill this goal.\n", "prob": 0.9}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Cross-check that each task node is compatible with the input types and output types required to successfully transform the image into text.\n", "prob": 0.8}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Validate that the task sequence results in the correct flow of data, ensuring outputs from one task serve as valid inputs to subsequent tasks.\n", "prob": 0.85}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the selected task for converting the image to text is among the specified tools in the task list, focusing on the 'Image-to-Text' task.\n", "prob": 0.9}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Confirm that the arguments used in the task align with the input types required by the chosen tool, verifying that 'example.jpg' is accepted as an image input.\n", "prob": 0.85}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Clue of Step 4:Check the overall structure of the generated plan to guarantee it follows the required format and accurately reflects the steps needed to resolve the user request.\n", "prob": 0.95}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the chosen task nodes are correctly defined in relation to the input requirements specified, namely the input type being an image and output type being text.\n", "prob": 0.9}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Verify that the constructed task links accurately reflect the order of operations required to transform the image to text and that all dependencies are respected.\n", "prob": 0.85}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Double-check that the descriptions in both the task steps and task nodes are aligned and clearly describe the intended outcomes based on the user's request.\n", "prob": 0.9}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\n", "value": 0.33333333283333333}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\n", "value": 0.6666666661666666}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\n", "value": 0.6666666661666666}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\nClue of Step 2:Identify the primary operation needed to convert the text content within the image to digitized text through the appropriate tool.\n", "value": 0.33333333283333333}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\nClue of Step 2:Ensure that the selected tool is capable of handling image input and producing text output, aligning with the user request.\n", "value": 0.33333333283333333}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\nClue of Step 2:Establish the sequence of actions in a logical flow from receiving the image input to producing the text output, making sure to consider any pre-processing if necessary.\n", "value": 0.6666666661666666}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\nClue of Step 2:Identify the necessary tasks to convert the image 'example.jpg' to text, primarily focusing on image-to-text conversion.\n", "value": 0.6666666661666666}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\nClue of Step 2:Ensure that the image input is properly formatted and available for the image-to-text model before executing the conversion task.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\nClue of Step 2:Sequence the tasks with a clear identification of the output of the image-to-text model that will serve as the final result of the user request.\n", "value": 0.6666666661666666}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\nClue of Step 2:Identify the specific sub-task required to convert an image containing text into its textual representation, which aligns with the 'Image to Text' classification task.\n", "value": 0.33333333283333333}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\nClue of Step 2:Determine the input required for the 'Image to Text' task, which is an image file, and confirm that the user has provided the necessary input ('example.jpg').\n", "value": 0.33333333283333333}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\nClue of Step 2:Establish that the output of the 'Image to Text' task serves the ultimate goal of converting the image into text, confirming that this output will be used for any further analysis or tasks, if required.\n", "value": 0.33333333283333333}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\nClue of Step 2:Identify the primary operation needed to convert the text content within the image to digitized text through the appropriate tool.\nClue of Step 3:Use Image to Text to extract text from the uploaded image file, as it is the primary task needed to achieve the user request.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key elements of the user request, which in this case involves processing an image that contains text and transforming that text into a readable format.\nClue of Step 2:Establish the sequence of actions in a logical flow from receiving the image input to producing the text output, making sure to consider any pre-processing if necessary.\n\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into sub-tasks that need to be executed sequentially, such as analyzing the image for text and converting it to text format.\nClue of Step 2:Identify the necessary tasks to convert the image 'example.jpg' to text, primarily focusing on image-to-text conversion.\nClue of Step 3:Verify that the task and its arguments align with the requirements defined in the task list, particularly for handling image inputs.\n", "value": 0.6666666661666666}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on selecting the appropriate tools based on the defined tasks related to image processing and text extraction.\nClue of Step 2:Identify the specific sub-task required to convert an image containing text into its textual representation, which aligns with the 'Image to Text' classification task.\nClue of Step 3:Plan to return the extracted text and make sure it meets the user's requirement of converting an image to text accurately.\n", "value": 0.33333333283333333}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [6, 21], [6, 22], [6, 23], [9, 24], [9, 25], [9, 26], [12, 27], [12, 28], [12, 29], [5, 30], [5, 31], [5, 32], [11, 33], [11, 34], [11, 35], [8, 36], [8, 37], [8, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [15, 45], [15, 46], [15, 47], [14, 48], [14, 49], [14, 50]], [[0, 1], [1, 3], [2, 2], [3, 4], [4, 7], [5, 10], [6, 6], [7, 9], [8, 12], [9, 5], [10, 11], [11, 8], [12, 13], [15, 16], [21, 15], [32, 14]], {"0": {"ucb": 0.9618372460323477, "p_ucb": 0.865653521429113, "var_p_ucb": 1.088476939982778, "action": "Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\n", "prob": 0.9}, "1": {"ucb": 1.1221434537044057, "p_ucb": 0.8977147629635246, "var_p_ucb": 1.128790900722881, "action": "Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\n", "prob": 0.8}, "2": {"ucb": 1.1221434537044057, "p_ucb": 0.9538219356487447, "var_p_ucb": 1.1993403320180611, "action": "Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\n", "prob": 0.85}, "3": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "Clue of Step 2:Extract key points from the article that mentions the best time to meditate, using summarization to condense the text.\n", "prob": 0.9}, "4": {"ucb": 1.8599451122392778, "p_ucb": 1.580953345403386, "var_p_ucb": 1.8132691689685119, "action": "Clue of Step 2:Use a question-answering model to specifically extract the answer to the question regarding the best time to meditate.\n", "prob": 0.85}, "5": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Conduct a conversational response to provide an engaging discussion around the best time to meditate based on extracted information.\n", "prob": 0.8}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Identify the need to summarize the article first to extract relevant information about the best time to meditate.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Determine that the extracted information about the best time to meditate can be used as input for generating conversational responses.\n", "prob": 0.85}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Recognize the need to use paraphrasing based on the conversational response generated, which should also reference the summarized information of the article.\n", "prob": 0.8}, "9": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Identify specific queries raised in the user's request, such as finding the best time to meditate and generating a paraphrase of the response.\n", "prob": 0.9}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Determine the necessary tools to extract information (summarization) and generate new text (text generation or conversational response modeling).\n", "prob": 0.85}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Map out the sequence of tasks required, ensuring that summarization precedes the conversation and paraphrasing of the extracted information.\n", "prob": 0.9}, "12": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Formulate the conversation and paraphrasing prompt based on the response received regarding the best time to meditate.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Utilize the Text Generation tool to create a paraphrased version of the response by rephrasing the text while retaining the original meaning.\n", "prob": 0.8}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Structure the interaction to ensure that the conversation yields relevant and coherent responses that enhance user understanding.\n", "prob": 0.85}, "15": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Identify potential conversational responses based on the user's needs for discussing the best time to meditate.\n", "prob": 0.9}, "16": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Consider paraphrasing the extracted answer to maintain the original meaning while providing variety in the expression.\n", "prob": 0.8}, "17": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Plan how to structure the final response to ensure it includes both a conversational aspect and the paraphrased answer.\n", "prob": 0.85}, "18": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Extract key information about the best time to meditate from the article through Summarization.\n", "prob": 0.9}, "19": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Engage in a conversational response using Conversational to discuss the extracted information about meditation timing.\n", "prob": 0.85}, "20": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Use Text Generation to paraphrase the response to provide a rephrased version of the best time to meditate.\n", "prob": 0.8}, "21": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Begin by summarizing the long article to encapsulate its main ideas regarding the best time to meditate.\n", "prob": 0.9}, "22": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Once the summary is produced, extract the specific information about the best time to meditate from the summarized text.\n", "prob": 0.85}, "23": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Generate a conversational response based on the extracted information and then paraphrase that response for clarity.\n", "prob": 0.8}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Begin by gathering and summarizing the key points of the article related to the benefits of meditation, which will provide a foundation for further extraction of specific information.\n", "prob": 0.85}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Extract the specific information related to 'the best time to meditate' from the summarized content, ensuring the focus is on the details most relevant to the user's query.\n", "prob": 0.9}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Utilize the extracted information to generate a coherent conversational response that reflects the findings from the article, while incorporating paraphrasing as needed to enhance clarity and engagement.\n", "prob": 0.8}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Summarize the article first to capture the key points about the best time to meditate.\n", "prob": 0.9}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Use the summarized information to extract the specific details regarding the best time to meditate.\n", "prob": 0.85}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Create a conversational response that replaces technical jargon with accessible language, using the extracted information.\n", "prob": 0.8}, "30": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Segment the process of identifying the best time to meditate into a specific task that extracts this information from the article while ensuring that the response remains clear and accurate.\n", "prob": 0.85}, "31": {"ucb": 4.19258829587282, "p_ucb": 3.144441221904615, "var_p_ucb": 3.4089455146191616, "action": "Clue of Step 3:Ensure that the paraphrasing task is designed to take the output from the best time identification and restate it in a coherent manner, aligning with the user's desire for a conversational response.\n", "prob": 0.75}, "32": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Organize the tasks such that the identification step is completed before the paraphrasing step to maintain logical flow and dependency management.\n", "prob": 0.9}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Identify the key points in the article about the best time to meditate by utilizing summarization tools.\n", "prob": 0.85}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Generate a response to the identified key points using conversational response modeling to create a natural dialogue.\n", "prob": 0.8}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Paraphrase the response using text generation tools to provide a unique presentation of the information.\n", "prob": 0.9}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Determine the necessary tasks for gathering information about the best time to meditate from the article through summarization.\n", "prob": 0.9}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Identify that the response from the summarization must be used for further conversational and paraphrasing tasks.\n", "prob": 0.8}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Create a sequence that begins with summarization, followed by conversational engagement and paraphrasing to ensure logical flow.\n", "prob": 0.85}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Verify that all the necessary tools and processes are aligned with the user's request to find the best time to meditate.\n", "prob": 0.85}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.497663833473093, "var_p_ucb": 2.6614882671121487, "action": "Clue of Step 4:Ensure that the summarization of the article is comprehensive enough to capture all relevant points on the best time to meditate.\n", "prob": 0.75}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Confirm that the conversation and paraphrasing task is based on a clear and direct response derived from the summarization.\n", "prob": 0.9}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Validate whether the question-answering task correctly retrieves the timing for meditation from the article.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the conversational and paraphrasing tasks are based on the output from the question-answering task, maintaining coherence.\n", "prob": 0.85}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Check that all generated outputs (timing, conversational response, and paraphrase) harmonize well together and fulfill the original request.\n", "prob": 0.9}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Ensure that the outputs of each sub-task are clear and well-defined, especially the summarization, as it will be the basis for the next steps.\n", "prob": 0.85}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Verify that the conversation and paraphrasing should draw from the summarized output effectively, ensuring relevant context is maintained.\n", "prob": 0.9}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Plan that the paraphrasing of the response follows after generating the conversational text to preserve coherence.\n", "prob": 0.8}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\nClue of Step 2:Extract key points from the article that mentions the best time to meditate, using summarization to condense the text.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\nClue of Step 2:Use a question-answering model to specifically extract the answer to the question regarding the best time to meditate.\n", "value": 0.0}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\nClue of Step 2:Conduct a conversational response to provide an engaging discussion around the best time to meditate based on extracted information.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\nClue of Step 2:Identify the need to summarize the article first to extract relevant information about the best time to meditate.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\nClue of Step 2:Determine that the extracted information about the best time to meditate can be used as input for generating conversational responses.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\nClue of Step 2:Recognize the need to use paraphrasing based on the conversational response generated, which should also reference the summarized information of the article.\n", "value": 0.0}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\nClue of Step 2:Identify specific queries raised in the user's request, such as finding the best time to meditate and generating a paraphrase of the response.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\nClue of Step 2:Determine the necessary tools to extract information (summarization) and generate new text (text generation or conversational response modeling).\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\nClue of Step 2:Map out the sequence of tasks required, ensuring that summarization precedes the conversation and paraphrasing of the extracted information.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\nClue of Step 2:Extract key points from the article that mentions the best time to meditate, using summarization to condense the text.\nClue of Step 3:Formulate the conversation and paraphrasing prompt based on the response received regarding the best time to meditate.\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the key information needed from the article, such as timing for meditation.\nClue of Step 2:Use a question-answering model to specifically extract the answer to the question regarding the best time to meditate.\nClue of Step 3:Identify potential conversational responses based on the user's needs for discussing the best time to meditate.\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break down the request into distinct tasks: summarizing the article, extracting specific information, generating conversational responses, and paraphrasing.\nClue of Step 2:Identify the need to summarize the article first to extract relevant information about the best time to meditate.\nClue of Step 3:Begin by summarizing the long article to encapsulate its main ideas regarding the best time to meditate.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Focus on understanding the user\u2019s intention and the main questions raised about meditation schedules in order to convert the high-level request into actionable tasks.\nClue of Step 2:Identify specific queries raised in the user's request, such as finding the best time to meditate and generating a paraphrase of the response.\nClue of Step 3:Organize the tasks such that the identification step is completed before the paraphrasing step to maintain logical flow and dependency management.\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [6, 12], [6, 13], [6, 14], [10, 15], [10, 16], [10, 17], [7, 18], [7, 19], [7, 20], [5, 21], [5, 22], [5, 23], [9, 24], [9, 25], [9, 26], [12, 27], [12, 28], [12, 29], [4, 30], [4, 31], [4, 32], [11, 33], [11, 34], [11, 35], [8, 36], [8, 37], [8, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [15, 45], [15, 46], [15, 47], [14, 48], [14, 49], [14, 50]], [[0, 1], [1, 3], [2, 2], [3, 6], [4, 10], [5, 7], [6, 5], [7, 9], [8, 12], [9, 4], [10, 11], [11, 8], [12, 13], [18, 16], [21, 15], [30, 14]], {"0": {"ucb": 1.5018372456273477, "p_ucb": 1.405653521024113, "var_p_ucb": 1.628476939577778, "action": "Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\n", "prob": 0.9}, "1": {"ucb": 1.7221434532544055, "p_ucb": 1.4977147625135245, "var_p_ucb": 1.7287909002728807, "action": "Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\n", "prob": 0.8}, "2": {"ucb": 1.7221434532544055, "p_ucb": 1.5538219351987448, "var_p_ucb": 1.799340331568061, "action": "Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\n", "prob": 0.85}, "3": {"ucb": 2.4599451117892777, "p_ucb": 2.27395060056535, "var_p_ucb": 2.519932060810777, "action": "Clue of Step 2:Determine the appropriate task to classify the image and establish the dependencies on the image input.\n", "prob": 0.9}, "4": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Identify the output of the image classification step, which will be a label for the image, necessary for completing any following steps.\n", "prob": 0.8}, "5": {"ucb": 2.4599451117892777, "p_ucb": 2.180953344953386, "var_p_ucb": 2.4132691685185117, "action": "Clue of Step 2:Consider whether additional tasks are needed for processing or utilizing the classification results, such as providing descriptions or connecting to a further action.\n", "prob": 0.85}, "6": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Identify that the main operation requires classifying the image from 'example.jpg', thus we initiate the Image Classification task.\n", "prob": 0.9}, "7": {"ucb": 3.277132397641701, "p_ucb": 2.8755625379279457, "var_p_ucb": 3.177431980408466, "action": "Clue of Step 2:Consider the specific requirements of the classification model regarding the input format (image format) and ensure the task nodes reflect that.\n", "prob": 0.85}, "8": {"ucb": 3.277132397641701, "p_ucb": 2.741705918023361, "var_p_ucb": 3.0258183344756153, "action": "Clue of Step 2:Establish that the output of the Image Classification task will provide the class label, which resolves the user request and does not need further actions.\n", "prob": 0.8}, "9": {"ucb": 2.3847549316111336, "p_ucb": 2.2062794384050206, "var_p_ucb": 2.419363750744211, "action": "Clue of Step 2:Identify the initial task of image classification and prepare to classify the image based on its content.\n", "prob": 0.9}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.141705918473361, "var_p_ucb": 2.4258183349256153, "action": "Clue of Step 2:Ensure that the output of the image classification task can be effectively used in any subsequent operations, such as generating textual descriptions or further visual processing.\n", "prob": 0.8}, "11": {"ucb": 3.277132397641701, "p_ucb": 2.8755625379279457, "var_p_ucb": 3.177431980408466, "action": "Clue of Step 2:Establish a sequence for identifying the class of the image and preparing for potential follow-up tasks like generating an image based on the classification or a textual description.\n", "prob": 0.85}, "12": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Select the tasked model that aligns with the goal of classifying the input image, ensuring that its input requirements are satisfied.\n", "prob": 0.9}, "13": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Establish the sequence of operations, identifying that the image must be fed into the chosen classification model.\n", "prob": 0.8}, "14": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Conclude by ensuring that all dependencies are accounted for before finalizing the task sequence.\n", "prob": 0.85}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "16": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "17": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "18": {"ucb": 2.69629414748641, "p_ucb": 2.486664732692769, "var_p_ucb": 2.645367308321497, "action": "Clue of Step 3:Coordinate the identification of the classification task based on the input image.\n", "prob": 0.9}, "19": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that the output of the image classification provides necessary information that might be useful for downstream tasks.\n", "prob": 0.8}, "20": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Prepare to implement the result of the image classification by linking it to any future actions based on the classification outcome.\n", "prob": 0.85}, "21": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Utilize the Image Classification task to produce a label identifying the class of the input image 'example.jpg'.\n", "prob": 0.9}, "22": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Ensure the output from the Image Classification task is stored to provide the classification result.\n", "prob": 0.85}, "23": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Confirm that the output of the Image Classification task is accurate and represents the intended classification of the image.\n", "prob": 0.8}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the specific task required to classify the image and return the appropriate label.\n", "prob": 0.9}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that the output from the image classification accurately corresponds to the input image and is understandable.\n", "prob": 0.8}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Verify that all steps leading to the final classification are clearly articulated and reflect the overall goal of the task.\n", "prob": 0.8}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Identify the input image task as the primary requirement and structure the task to classify the image.\n", "prob": 0.9}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that the output from the Image Classification task is directly linked to the user request for the class it belongs to.\n", "prob": 0.8}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Formulate a straightforward task sequence where the image classification directly resolves the user's query without additional dependencies.\n", "prob": 0.85}, "30": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Use the output from the image classification task to determine what kind of class the image belongs to.\n", "prob": 0.9}, "31": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "Clue of Step 3:Ensure that the arguments for the task nodes reflect the input required for the image classification task.\n", "prob": 0.8}, "32": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Link the classification result back to the user request to confirm that the task has been completed successfully.\n", "prob": 0.85}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Start with processing the image using the Image Classification task to determine the class of the image.\n", "prob": 0.9}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Ensure that once the image is classified, the result can be used for any subsequent tasks that may require contextual understanding or further manipulation.\n", "prob": 0.8}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Identify if an output from the Image Classification task leads to other actions, such as generating text, image modifications, or additional analysis.\n", "prob": 0.85}, "36": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Determine that the first action is to classify the image in order to know its class for potential follow-up actions, thus establishing a primary dependency.\n", "prob": 0.9}, "37": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Confirm that the output from the image classification task will inform any further analyses or manipulations that might be required, ensuring continuity in the workflow.\n", "prob": 0.85}, "38": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Plan to maintain a clear linkage between identified classes of the image and the application of subsequent tasks like image generation or editing, based on the results of the classification.\n", "prob": 0.8}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that all input and output requirements are met for the selected tool by checking alignment with the task list specifications.\n", "prob": 0.9}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Confirm that the output of the image classification task directly addresses the user request for class identification.\n", "prob": 0.85}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Review dependencies between the classification task and any potential follow-up tasks to ensure a cohesive approach.\n", "prob": 0.8}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the output of the Image Classification task is linked and contributes directly to understanding the class of the image.\n", "prob": 0.9}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Confirm that the arguments for the Image Classification task correctly reflect the input type requirements, specifically an image input.\n", "prob": 0.8}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Establish the flow of information by relating the output from the Image Classification task to possible further analysis or responses, if necessary.\n", "prob": 0.85}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the task nodes reflect the logical sequence where each node's output is relevantly consumed by the subsequent nodes, focusing on the image classification process first.\n", "prob": 0.9}, "49": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Make sure that each task node aligns with the provided task list, confirming that the chosen tasks can process appropriate inputs and yield expected outputs.\n", "prob": 0.85}, "50": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Double-check the consistency between task steps and task nodes, ensuring that every task step corresponds accurately to its defined node while facilitating a coherent flow from input to output.\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\n", "value": 0.6666666661666666}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\n", "value": 0.6666666661666666}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\nClue of Step 2:Determine the appropriate task to classify the image and establish the dependencies on the image input.\n", "value": 0.6666666661666666}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\nClue of Step 2:Identify the output of the image classification step, which will be a label for the image, necessary for completing any following steps.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\nClue of Step 2:Consider whether additional tasks are needed for processing or utilizing the classification results, such as providing descriptions or connecting to a further action.\n", "value": 0.6666666661666666}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\nClue of Step 2:Identify that the main operation requires classifying the image from 'example.jpg', thus we initiate the Image Classification task.\n", "value": 0.6666666661666666}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\nClue of Step 2:Consider the specific requirements of the classification model regarding the input format (image format) and ensure the task nodes reflect that.\n", "value": 0.6666666661666666}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\nClue of Step 2:Establish that the output of the Image Classification task will provide the class label, which resolves the user request and does not need further actions.\n", "value": 0.6666666661666666}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\nClue of Step 2:Identify the initial task of image classification and prepare to classify the image based on its content.\n", "value": 0.6666666661666666}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\nClue of Step 2:Ensure that the output of the image classification task can be effectively used in any subsequent operations, such as generating textual descriptions or further visual processing.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\nClue of Step 2:Establish a sequence for identifying the class of the image and preparing for potential follow-up tasks like generating an image based on the classification or a textual description.\n", "value": 0.6666666661666666}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\nClue of Step 2:Determine the appropriate task to classify the image and establish the dependencies on the image input.\nClue of Step 3:Select the tasked model that aligns with the goal of classifying the input image, ensuring that its input requirements are satisfied.\n", "value": 0.6666666661666666}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main goal of the user request, which is to classify an image, and outline the initial steps needed to accomplish that.\nClue of Step 2:Consider whether additional tasks are needed for processing or utilizing the classification results, such as providing descriptions or connecting to a further action.\nClue of Step 3:Coordinate the identification of the classification task based on the input image.\n", "value": 0.6666666661666666}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Determine that the first major task involves using an image classification model to analyze the input image and produce an output.\nClue of Step 2:Identify that the main operation requires classifying the image from 'example.jpg', thus we initiate the Image Classification task.\nClue of Step 3:Utilize the Image Classification task to produce a label identifying the class of the input image 'example.jpg'.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Recognize that the output from the image classification task will be used as input for any subsequent tasks, ensuring a logical flow.\nClue of Step 2:Identify the initial task of image classification and prepare to classify the image based on its content.\nClue of Step 3:Use the output from the image classification task to determine what kind of class the image belongs to.\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [3, 6], [3, 7], [3, 8], [2, 9], [2, 10], [2, 11], [8, 12], [8, 13], [8, 14], [11, 15], [11, 16], [11, 17], [5, 18], [5, 19], [5, 20], [6, 21], [6, 22], [6, 23], [12, 24], [12, 25], [12, 26], [9, 27], [9, 28], [9, 29], [4, 30], [4, 31], [4, 32], [10, 33], [10, 34], [10, 35], [7, 36], [7, 37], [7, 38], [14, 39], [14, 40], [14, 41], [15, 42], [13, 43], [13, 44], [13, 45], [16, 46], [16, 47], [16, 48]], [[0, 1], [1, 3], [2, 2], [3, 8], [4, 11], [5, 5], [6, 6], [7, 12], [8, 9], [9, 4], [10, 10], [11, 7], [20, 14], [22, 15], [30, 13], [36, 16]], {"0": {"ucb": 1.1221434537044057, "p_ucb": 1.0099291083339652, "var_p_ucb": 1.269889763313241, "action": "Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\n", "prob": 0.9}, "1": {"ucb": 1.1221434537044057, "p_ucb": 0.9538219356487447, "var_p_ucb": 1.1993403320180611, "action": "Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\n", "prob": 0.85}, "2": {"ucb": 0.9618372460323477, "p_ucb": 0.9137453837307302, "var_p_ucb": 1.1489478810929323, "action": "Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\n", "prob": 0.95}, "3": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Break down the user request into clear subtasks: detect emotion from audio, answer the question based on the image and emotion, generate descriptive text, and perform named entity recognition and part-of-speech tagging on that text.\n", "prob": 0.9}, "4": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Ensure that the dependencies between the identified subtasks are correctly established; for instance, the emotion detection must precede the visual question answering, and the answering task should come before text generation.\n", "prob": 0.85}, "5": {"ucb": 1.7847549320611338, "p_ucb": 1.695517185458077, "var_p_ucb": 1.9204395151494456, "action": "Clue of Step 2:Assign specific tasks from the task list that correspond to each of the identified subtasks while maintaining the order of execution based on their dependencies.\n", "prob": 0.95}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.6062794388550206, "var_p_ucb": 1.8193637511942111, "action": "Clue of Step 2:Identify the core functionalities needed from the audio and image inputs, then map those functionalities to appropriate tasks.\n", "prob": 0.9}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.2755625383779456, "var_p_ucb": 2.577431980858466, "action": "Clue of Step 2:Check for any additional outputs that may be generated from the tasks, ensuring they serve the overall user request effectively.\n", "prob": 0.85}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.409419158282531, "var_p_ucb": 2.729045626791317, "action": "Clue of Step 2:Verify that chosen tasks cover all necessary components of the user request without omission, ensuring a comprehensive solution.\n", "prob": 0.9}, "9": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "Clue of Step 2:Identify key emotions expressed in the voice recording, which will be necessary for answering the later question about feelings based on the image.\n", "prob": 0.9}, "10": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "Clue of Step 2:Ensure that the visual question answering task accurately processes the image and takes the detected emotion into account when forming an answer.\n", "prob": 0.8}, "11": {"ucb": 1.8599451122392778, "p_ucb": 1.580953345403386, "var_p_ucb": 1.8132691689685119, "action": "Clue of Step 2:After the emotional analysis and visual question answering, focus on generating a summarized text that incorporates insights from the previous tasks.\n", "prob": 0.85}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Map out the defined subtasks such that they align with the tools from the task list, ensuring that each task corresponds to the actual tools used for each identified component (emotion detection, visual question answering, text generation, and NLP tasks).\n", "prob": 0.85}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Create a sequence for the subtasks that respects their dependencies and flows logically, ensuring the output of one task serves as the input for the next task, like linking emotion detection results to the visual question answering.\n", "prob": 0.9}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Verify the inputs and outputs of each task to ensure they conform to the expected formats specified in the task list, thereby facilitating smooth transitions from one task to another as per the user request.\n", "prob": 0.8}, "18": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Formulate a sequence of structured tasks that properly address each component of the user request while ensuring that the input and output transitions seamlessly among them.\n", "prob": 0.9}, "19": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Review the selected tasks to ensure logical alignment with the identified subtasks and comply with the required input-output specifications.\n", "prob": 0.85}, "20": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Clue of Step 3:Verify that all dependencies and task links accurately reflect the required flow of information from one task to the next, confirming the objective is met.\n", "prob": 0.95}, "21": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Analyze the outputs required from each sub-task and create appropriate mappings to the tools for processing the audio and image inputs.\n", "prob": 0.9}, "22": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Clue of Step 3:Ensure that the outputs of one task can be effectively used as inputs for the subsequent tasks, maintaining a clear flow of information.\n", "prob": 0.95}, "23": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Prioritize the execution of tasks based on their dependencies, ensuring that each step is performed in the correct order.\n", "prob": 0.85}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Determine the initial sequence of sub-tasks starting with Audio Classification to analyze the emotion in the audio recording.\n", "prob": 0.9}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Identify that the output of Audio Classification should inform the input to the Visual Question Answering task, ensuring the emotion detected is used for context in the question posed about the image.\n", "prob": 0.8}, "26": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Recognize that after obtaining the answer from Visual Question Answering, a summarization task should be applied to generate a concise description of the answer, followed by token classification to analyze the summary text.\n", "prob": 0.85}, "27": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 3:Map the identified sub-tasks to the corresponding tools from the task list, ensuring that each selected tool aligns with the required input and output types.\n", "prob": 0.9}, "28": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 3:Establish a sequential order for task execution based on the dependencies recognized, ensuring output from one task is an appropriate input for the subsequent task.\n", "prob": 0.8}, "29": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 3:Ensure the entire flow of tasks provides a cohesive solution that effectively addresses the user's request while maintaining clarity and logical progression.\n", "prob": 0.85}, "30": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "36": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Clue of Step 3:Determine the appropriate sequence for audio analysis, image-based questioning, text generation, and text analysis to fulfill the user's request.\n", "prob": 0.9}, "37": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Clue of Step 3:Identify the key data outputs from each sub-task that feed into the next, ensuring that the data flows logically and efficiently through the tasks.\n", "prob": 0.85}, "38": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Clue of Step 3:Confirm that all dependencies are respected, ensuring that each step logically progresses towards addressing the user\u2019s overall request.\n", "prob": 0.9}, "39": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the selected tasks align with the input and output requirements specified in the task list, confirming each task accepts the correct format for its inputs.\n", "prob": 0.9}, "40": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Review the dependencies established between tasks, ensuring that the outputs from one task correctly feed into the subsequent tasks without data loss or misalignment.\n", "prob": 0.85}, "41": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Cross-check that the sequence of tasks not only meets the user requirement but also maintains efficiency, avoiding any unnecessary tasks or redundancies.\n", "prob": 0.8}, "42": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Ensure the sequence of tasks reflects the logical flow of information between the audio analysis, question answering, text generation, and token classification.\n", "prob": 0.8}, "43": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure that the identified emotions from the audio are effectively utilized to respond to the contextual question regarding feelings based on the image.\n", "prob": 0.9}, "44": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Confirm that the generated descriptive text summarizes the insights derived from both the emotion detection and the visual content accurately.\n", "prob": 0.85}, "45": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Verify that the token classification effectively extracts names and relevant parts of speech from the descriptive text to enhance the understanding of its components.\n", "prob": 0.8}, "46": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Clue of Step 4:Identify the appropriate models needed for emotion detection, visual question answering, text summarization, and token classification directly linked to the outputs of previous steps.\n", "prob": 0.85}, "47": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Clue of Step 4:Ensure each sub-task outputs values that are directly usable as inputs for following tasks, thereby creating a clear flow from one task to the next.\n", "prob": 0.9}, "48": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Clue of Step 4:Confirm that the generated text is structured correctly for named entity recognition and part-of-speech tagging for the final analysis step.\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\n", "value": 0.0}, "8": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\nClue of Step 2:Break down the user request into clear subtasks: detect emotion from audio, answer the question based on the image and emotion, generate descriptive text, and perform named entity recognition and part-of-speech tagging on that text.\n", "value": 0.0}, "11": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\nClue of Step 2:Ensure that the dependencies between the identified subtasks are correctly established; for instance, the emotion detection must precede the visual question answering, and the answering task should come before text generation.\n", "value": 0.0}, "5": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\nClue of Step 2:Assign specific tasks from the task list that correspond to each of the identified subtasks while maintaining the order of execution based on their dependencies.\n", "value": 0.0}, "6": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\nClue of Step 2:Identify the core functionalities needed from the audio and image inputs, then map those functionalities to appropriate tasks.\n", "value": 0.0}, "12": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\nClue of Step 2:Check for any additional outputs that may be generated from the tasks, ensuring they serve the overall user request effectively.\n", "value": 0.0}, "9": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\nClue of Step 2:Verify that chosen tasks cover all necessary components of the user request without omission, ensuring a comprehensive solution.\n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\nClue of Step 2:Identify key emotions expressed in the voice recording, which will be necessary for answering the later question about feelings based on the image.\n", "value": 0.0}, "10": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\nClue of Step 2:Ensure that the visual question answering task accurately processes the image and takes the detected emotion into account when forming an answer.\n", "value": 0.0}, "7": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\nClue of Step 2:After the emotional analysis and visual question answering, focus on generating a summarized text that incorporates insights from the previous tasks.\n", "value": 0.0}, "14": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Identify the main components of the user request, which include emotion detection from audio, visual question answering based on an image, text generation summarizing the answer, and named entity recognition and part-of-speech tagging of the generated text.\nClue of Step 2:Assign specific tasks from the task list that correspond to each of the identified subtasks while maintaining the order of execution based on their dependencies.\nClue of Step 3:Verify that all dependencies and task links accurately reflect the required flow of information from one task to the next, confirming the objective is met.\n", "value": 0.0}, "15": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Break the request into four sub-tasks: analyzing the audio for emotion, asking a visual question based on the image and detected emotion, generating a text description of the response, and performing token classification on the generated text.\nClue of Step 2:Identify the core functionalities needed from the audio and image inputs, then map those functionalities to appropriate tasks.\nClue of Step 3:Ensure that the outputs of one task can be effectively used as inputs for the subsequent tasks, maintaining a clear flow of information.\n", "value": 0.0}, "13": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\nClue of Step 2:Identify key emotions expressed in the voice recording, which will be necessary for answering the later question about feelings based on the image.\n\n", "value": 0.0}, "16": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: {\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step clues for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Clue of Step 1\": \"Identify the high-level goal or main request and determine the general sub-tasks required to achieve it.\"},\n    {\"Clue of Step 2\": \"Analyze the dependencies between sub-tasks and construct a graph where nodes represent sub-tasks and edges represent dependencies.\"},\n    {\"Clue of Step 3\": \"Select an optimal path within the graph, ensuring all dependencies are respected and that the path solves the problem efficiently.\"},\n    {\"Clue of Step 4\": \"Validate the sub-tasks by checking if all necessary components are addressed in the solution.\"}\n    ]\n    ```\n    Based on this example, generate step-by-step clues to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Clues-----\n    Clue of Step 1:Ensure that the defined sub-tasks logically represent the user's needs and are suitable for processing individual steps of audio, image, text generation, and natural language analysis.\nClue of Step 2:After the emotional analysis and visual question answering, focus on generating a summarized text that incorporates insights from the previous tasks.\nClue of Step 3:Determine the appropriate sequence for audio analysis, image-based questioning, text generation, and text analysis to fulfill the user's request.\n", "value": 0.0}}]
