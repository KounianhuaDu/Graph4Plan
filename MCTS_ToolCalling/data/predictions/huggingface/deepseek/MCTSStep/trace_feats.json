[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [5, 21], [5, 22], [5, 23], [8, 24], [8, 25], [8, 26], [11, 27], [11, 28], [11, 29], [6, 30], [6, 31], [6, 32], [9, 33], [9, 34], [9, 35], [12, 36], [12, 37], [12, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 10], [6, 5], [7, 8], [8, 11], [9, 6], [10, 9], [11, 12], [12, 13], [15, 16], [21, 14], [30, 15]], {"0": {"ucb": 1.2618372455823477, "p_ucb": 1.2618372455823477, "var_p_ucb": 1.5094188217530868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "4": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 3.0899176679089164, "p_ucb": 3.0899176679089164, "var_p_ucb": 3.499886768317962, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [5, 21], [5, 22], [5, 23], [8, 24], [8, 25], [8, 26], [11, 27], [11, 28], [11, 29], [6, 30], [6, 31], [6, 32], [9, 33], [9, 34], [9, 35], [12, 36], [12, 37], [12, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 10], [6, 5], [7, 8], [8, 11], [9, 6], [10, 9], [11, 12], [12, 13], [15, 16], [21, 14], [30, 15]], {"0": {"ucb": 1.2618372455823477, "p_ucb": 1.2618372455823477, "var_p_ucb": 1.5094188217530868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "4": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 3.0899176679089164, "p_ucb": 3.0899176679089164, "var_p_ucb": 3.499886768317962, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [2, 5], [2, 6], [3, 7], [3, 8], [4, 9], [7, 10], [10, 11], [10, 12], [10, 13], [5, 14], [8, 15], [11, 16], [11, 17], [11, 18], [6, 19], [9, 20], [12, 21], [12, 22], [12, 23], [13, 24], [13, 25], [13, 26], [16, 27], [16, 28], [16, 29], [14, 30], [14, 31], [14, 32], [15, 33], [15, 34], [15, 35]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 5], [6, 8], [7, 6], [8, 9], [9, 13], [10, 16], [14, 14], [19, 15]], {"0": {"ucb": 1.2618372455823477, "p_ucb": 1.2618372455823477, "var_p_ucb": 1.5094188217530868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 2.159945111789278, "p_ucb": 2.066947856177314, "var_p_ucb": 2.3265949531030428, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "4": {"ucb": 2.159945111789278, "p_ucb": 1.97395060056535, "var_p_ucb": 2.2199320608107773, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "prob": 0.9}, "5": {"ucb": 2.084754931611134, "p_ucb": 1.995517185008077, "var_p_ucb": 2.2204395146994456, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "6": {"ucb": 2.977132397641701, "p_ucb": 2.7094191578325306, "var_p_ucb": 3.0290456263413166, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "prob": 0.9}, "7": {"ucb": 2.084754931611134, "p_ucb": 1.995517185008077, "var_p_ucb": 2.2204395146994456, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "8": {"ucb": 2.977132397641701, "p_ucb": 2.7094191578325306, "var_p_ucb": 3.0290456263413166, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "prob": 0.9}, "9": {"ucb": 2.39629414748641, "p_ucb": 2.186664732692769, "var_p_ucb": 2.3453673083214968, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.9}, "10": {"ucb": 2.39629414748641, "p_ucb": 2.2914794400895895, "var_p_ucb": 2.4589988254754687, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 2.39629414748641, "p_ucb": 2.186664732692769, "var_p_ucb": 2.3453673083214968, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.9}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "16": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 2.39629414748641, "p_ucb": 2.186664732692769, "var_p_ucb": 2.3453673083214968, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.9}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "21": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "value": 0.33333333283333333}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.33333333283333333}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "value": 0.33333333283333333}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.33333333283333333}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n", "value": 0.33333333283333333}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.33333333283333333}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'audio file.mp3' and output: 'enhanced audio file.mp3'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.33333333283333333}}]
[[[0, 0], [0, 1], [0, 2], [0, 3], [1, 4], [1, 5], [1, 6], [2, 7], [2, 8], [2, 9], [3, 10], [3, 11], [3, 12], [4, 13], [4, 14], [5, 15], [5, 16], [5, 17], [9, 18], [9, 19], [9, 20], [13, 21], [13, 22], [13, 23], [6, 24], [6, 25], [6, 26], [10, 27], [10, 28], [10, 29], [14, 30], [14, 31], [14, 32], [7, 33], [7, 34], [7, 35], [11, 36], [11, 37], [11, 38], [15, 39], [15, 40], [15, 41], [8, 42], [8, 43], [8, 44], [12, 45], [12, 46], [12, 47], [16, 48], [16, 49], [16, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 9], [6, 13], [7, 6], [8, 10], [9, 14], [10, 7], [11, 11], [12, 15], [13, 8], [14, 12]], {"0": {"ucb": 1.3465721444452867, "p_ucb": 1.2119149300007581, "var_p_ucb": 1.5238677159758893, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 1.3465721444452867, "p_ucb": 1.2119149300007581, "var_p_ucb": 1.5238677159758893, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "2": {"ucb": 1.3465721444452867, "p_ucb": 1.1445863227784936, "var_p_ucb": 1.4392083984216733, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "3": {"ucb": 1.3465721444452867, "p_ucb": 1.0772577155562295, "var_p_ucb": 1.3545490808674572, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n", "prob": 0.8}, "4": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 2.537272482359039, "p_ucb": 2.410408858241087, "var_p_ucb": 2.6936340860759325, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n", "prob": 0.95}, "14": {"ucb": 2.537272482359039, "p_ucb": 2.2835452341231353, "var_p_ucb": 2.5518638710193047, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'enhanced narration audio' and output: 'enhanced audio file'\n", "prob": 0.9}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n", "value": 0.0}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n", "value": 0.0}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Audio Editing tool with input: 'enhanced narration audio' and output: 'enhanced audio file'\n", "value": 0.0}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Speech tool with input: 'Enhance the speech quality of the narration' and output: 'enhanced narration audio'\n<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.0}}]
[[[0, 0], [0, 1], [1, 2], [1, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 8], [3, 9], [4, 10], [4, 11], [4, 12], [7, 13], [7, 14], [7, 15], [5, 16], [5, 17], [5, 18], [8, 19], [8, 20], [8, 21], [8, 22], [11, 23], [11, 24], [11, 25], [14, 26], [14, 27], [14, 28], [6, 29], [6, 30], [9, 31], [9, 32], [9, 33], [13, 34], [13, 35], [10, 36], [10, 37], [10, 38], [15, 39], [15, 40], [15, 41], [12, 42], [12, 43], [12, 44], [16, 45], [16, 46], [16, 47]], [[0, 1], [1, 2], [2, 4], [3, 7], [4, 5], [5, 8], [6, 11], [7, 14], [8, 6], [9, 9], [10, 10], [11, 15], [13, 12], [16, 16]], {"0": {"ucb": 0.9618372460323477, "p_ucb": 0.865653521429113, "var_p_ucb": 1.088476939982778, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 0.9618372460323477, "p_ucb": 0.8175616591274955, "var_p_ucb": 1.0280059988726238, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 1.3949588341794583, "p_ucb": 1.3252108924704853, "var_p_ucb": 1.519946215164782, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n", "prob": 0.95}, "3": {"ucb": 1.8599451122392778, "p_ucb": 1.67395060101535, "var_p_ucb": 1.9199320612607773, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'label for each object in the image' and output: 'spoken labels for each object'\n", "prob": 0.9}, "4": {"ucb": 1.8599451122392778, "p_ucb": 1.7669478566273138, "var_p_ucb": 2.0265949535530425, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Object Detection tool with input: 'example.jpg' and output: 'object detection results'\n", "prob": 0.95}, "5": {"ucb": 2.7899176683589166, "p_ucb": 2.510925901523025, "var_p_ucb": 2.879898091891166, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the objects detected in the image' and output: 'description image'\n", "prob": 0.9}, "6": {"ucb": 2.7899176683589166, "p_ucb": 2.371430018105079, "var_p_ucb": 2.7199037534527677, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Editing tool with input: 'description image' and output: 'edited description image'\n", "prob": 0.85}, "7": {"ucb": 2.7899176683589166, "p_ucb": 2.2319341346871333, "var_p_ucb": 2.5599094150143697, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Sentence Similarity tool with input: ['description of objects from Image Editing', 'question text'] and output: 'similarity score'\n", "prob": 0.8}, "8": {"ucb": 2.537272482359039, "p_ucb": 2.2835452341231353, "var_p_ucb": 2.5518638710193047, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "9": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "10": {"ucb": 2.3548200450309493, "p_ucb": 2.3548200450309493, "var_p_ucb": 2.593519386064386, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.3548200450309493, "p_ucb": 2.3548200450309493, "var_p_ucb": 2.593519386064386, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 4.709640090061899, "p_ucb": 4.709640090061899, "var_p_ucb": 5.187038772128772, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Classification tool with input: 'edited thumbnail image' and output: 'object categories'\n", "prob": 0.95}, "20": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Image tool with input: 'Describe the objects detected in the image' and output: 'description image'\n", "prob": 0.9}, "21": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Editing tool with input: 'description image' and output: 'enhanced description image'\n", "prob": 0.85}, "22": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'enhanced description image' and output: 'spoken description'\n", "prob": 0.8}, "23": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "24": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "25": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Object Detection tool with input: 'thumbnail image' and output: 'object labels'\n", "prob": 0.8}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Classification tool with input: 'example.jpg' and output: 'object labels'\n", "prob": 0.95}, "30": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Speech tool with input: 'What is the most commonly found object in households? An object commonly found in households is a chair.' and output: 'speech audio'\n", "prob": 0.9}, "31": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Image tool with input: 'example.jpg' and output: 'description image'\n", "prob": 0.95}, "32": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Classification tool with input: 'description image' and output: 'object categories'\n", "prob": 0.9}, "33": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Sentence Similarity tool with input: ['description image object categories', 'What is the most commonly found object in households? An object commonly found in households is a chair.'] and output: 'similarity score'\n", "prob": 0.85}, "34": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'edited thumbnail image'\n", "prob": 0.9}, "35": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 3:Call Image Editing tool with input: 'edited thumbnail image' and output: 'final image'\n", "prob": 0.85}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.0}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n", "value": 0.0}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'label for each object in the image' and output: 'spoken labels for each object'\n", "value": 0.0}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Object Detection tool with input: 'example.jpg' and output: 'object detection results'\n", "value": 0.0}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the objects detected in the image' and output: 'description image'\n", "value": 0.0}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Editing tool with input: 'description image' and output: 'edited description image'\n", "value": 0.0}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Sentence Similarity tool with input: ['description of objects from Image Editing', 'question text'] and output: 'similarity score'\n", "value": 0.0}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c><\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c><\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c><\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>", "value": 0.0}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Text-to-Speech tool with input: 'label for each object in the image' and output: 'spoken labels for each object'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>Tool Call of Step 2:Call Object Detection tool with input: 'example.jpg' and output: 'object detection results'\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [5, 21], [5, 22], [5, 23], [8, 24], [8, 25], [8, 26], [11, 27], [11, 28], [11, 29], [6, 30], [6, 31], [6, 32], [9, 33], [9, 34], [9, 35], [12, 36], [12, 37], [12, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 10], [6, 5], [7, 8], [8, 11], [9, 6], [10, 9], [11, 12], [12, 13], [15, 16], [21, 14], [30, 15]], {"0": {"ucb": 0.9618372460323477, "p_ucb": 0.9618372460323477, "var_p_ucb": 1.2094188222030868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.1221434537044057, "p_ucb": 1.1221434537044057, "var_p_ucb": 1.4109886259036013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.1221434537044057, "p_ucb": 1.1221434537044057, "var_p_ucb": 1.4109886259036013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 1.8599451122392778, "p_ucb": 1.8599451122392778, "var_p_ucb": 2.1332578458453084, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "4": {"ucb": 1.8599451122392778, "p_ucb": 1.8599451122392778, "var_p_ucb": 2.1332578458453084, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 2.7899176683589166, "p_ucb": 2.7899176683589166, "var_p_ucb": 3.1998867687679624, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.7847549320611338, "var_p_ucb": 2.0215152791046793, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 1.7847549320611338, "p_ucb": 1.7847549320611338, "var_p_ucb": 2.0215152791046793, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [5, 21], [5, 22], [5, 23], [8, 24], [8, 25], [8, 26], [11, 27], [11, 28], [11, 29], [6, 30], [6, 31], [6, 32], [9, 33], [9, 34], [9, 35], [12, 36], [12, 37], [12, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 10], [6, 5], [7, 8], [8, 11], [9, 6], [10, 9], [11, 12], [12, 13], [15, 16], [21, 14], [30, 15]], {"0": {"ucb": 1.2618372455823477, "p_ucb": 1.2618372455823477, "var_p_ucb": 1.5094188217530868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.4221434532544057, "p_ucb": 1.4221434532544057, "var_p_ucb": 1.7109886254536013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "4": {"ucb": 2.159945111789278, "p_ucb": 2.159945111789278, "var_p_ucb": 2.4332578453953086, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 3.0899176679089164, "p_ucb": 3.0899176679089164, "var_p_ucb": 3.499886768317962, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 2.084754931611134, "p_ucb": 2.084754931611134, "var_p_ucb": 2.3215152786546795, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.977132397641701, "p_ucb": 2.977132397641701, "var_p_ucb": 3.3322729182070194, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 2.39629414748641, "p_ucb": 2.39629414748641, "var_p_ucb": 2.5726303426294415, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.33333333283333333}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14], [7, 15], [7, 16], [7, 17], [10, 18], [10, 19], [10, 20], [5, 21], [5, 22], [5, 23], [8, 24], [8, 25], [8, 26], [11, 27], [11, 28], [11, 29], [6, 30], [6, 31], [6, 32], [9, 33], [9, 34], [9, 35], [12, 36], [12, 37], [12, 38], [13, 39], [13, 40], [13, 41], [16, 42], [16, 43], [16, 44], [14, 45], [14, 46], [14, 47], [15, 48], [15, 49], [15, 50]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 7], [5, 10], [6, 5], [7, 8], [8, 11], [9, 6], [10, 9], [11, 12], [12, 13], [15, 16], [21, 14], [30, 15]], {"0": {"ucb": 0.9618372460323477, "p_ucb": 0.9618372460323477, "var_p_ucb": 1.2094188222030868, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "1": {"ucb": 1.1221434537044057, "p_ucb": 1.1221434537044057, "var_p_ucb": 1.4109886259036013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "2": {"ucb": 1.1221434537044057, "p_ucb": 1.1221434537044057, "var_p_ucb": 1.4109886259036013, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "3": {"ucb": 1.8599451122392778, "p_ucb": 1.8599451122392778, "var_p_ucb": 2.1332578458453084, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "4": {"ucb": 1.8599451122392778, "p_ucb": 1.8599451122392778, "var_p_ucb": 2.1332578458453084, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "5": {"ucb": 2.7899176683589166, "p_ucb": 2.7899176683589166, "var_p_ucb": 3.1998867687679624, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "6": {"ucb": 1.7847549320611338, "p_ucb": 1.7847549320611338, "var_p_ucb": 2.0215152791046793, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "7": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "8": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "9": {"ucb": 1.7847549320611338, "p_ucb": 1.7847549320611338, "var_p_ucb": 2.0215152791046793, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "10": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "11": {"ucb": 2.677132398091701, "p_ucb": 2.677132398091701, "var_p_ucb": 3.032272918657019, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "12": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "13": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "14": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "15": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "16": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "17": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "19": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "20": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "21": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "22": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "23": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "24": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "25": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "26": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "27": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "28": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "29": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "30": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "31": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "32": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "33": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "34": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "35": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "36": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "37": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "38": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "39": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "40": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "41": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "42": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "43": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "44": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "45": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "46": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "47": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "48": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "49": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}, "50": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "prob": 1.0}}, {"0": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "2": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "3": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "4": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "7": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "10": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "5": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "8": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "11": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "6": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "9": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "12": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "13": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "16": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "14": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}, "15": {"state": "<\uff5cbegin\u2581of\u2581sentence\uff5c>\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    <\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n<\uff5cbegin\u2581of\u2581sentence\uff5c>\n", "value": 0.0}}]
