[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.6915149882393594, "var_p_ucb": 1.8902695340883735, "action": "\n", "prob": 1.0}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "2": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "3": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "\n", "prob": 1.0}, "4": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "5": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "7": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "8": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please help me classify the content of the given audio file 'example.wav'.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 2.09629414793641, "var_p_ucb": 2.2726303430794412, "action": "\n", "prob": 1.0}, "4": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "5": {"ucb": 4.19258829587282, "p_ucb": 4.19258829587282, "var_p_ucb": 4.5452606861588825, "action": "\n", "prob": 1.0}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "7": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "8": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I need a conversational response for the following question: 'What are the benefits of exercising regularly?'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.6069392388273913, "var_p_ucb": 1.795756057383955, "action": "Tool Call of Step 1:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.2835452341231353, "var_p_ucb": 2.5518638710193047, "action": "Tool Call of Step 1:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file2.mp3'\n", "prob": 0.9}, "2": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "", "prob": 0.85}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.982958881079179, "var_p_ucb": 4.317997651850938, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "5": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "", "prob": 0.9}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file2.mp3'\n", "prob": 0.95}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file3.mp3'\n", "prob": 0.9}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "", "prob": 0.85}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "10": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file2.mp3'\n", "prob": 0.9}, "11": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "", "prob": 0.85}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "prob": 0.95}, "14": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "", "prob": 0.9}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file2.mp3'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Please convert the following text to an audio file: 'The quick brown fox jumps over the lazy dog.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\nTool Call of Step 2:Call Text-to-Speech tool with input: 'The quick brown fox jumps over the lazy dog.' and output: 'audio file.mp3'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "5": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "", "prob": 0.7}, "6": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: Create a video summarizing example.jpg document image about the topic 'environmental protection' and enhance the speech quality of the narration\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [2, 5], [2, 6], [2, 7], [3, 8], [3, 9], [3, 10], [4, 11], [4, 12], [4, 13]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n", "prob": 0.95}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Tool Call of Step 2:Call Sentence Similarity with input: ['label for each object in the image', 'What is the most commonly found object in households? An object commonly found in households is a chair.'], output: 'similarity score'\n", "prob": 0.9}, "5": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "7": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "9": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "10": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "11": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n", "prob": 0.9}, "12": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 3:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.85}, "13": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Tool Call of Step 3:Call Object Detection tool with input: 'thumbnail image' and output: 'bounding boxes and labels for each object detected in the image'\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', which contains different objects. Please help me segment these objects, label them, and answer questions based on the provided text: 'What is the most commonly found object in households? An object commonly found in households is a chair.'\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'label for each object in the image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [4, 11], [4, 12], [4, 13], [4, 14], [4, 15], [4, 16], [4, 17], [4, 18], [4, 19]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'edited thumbnail image' and output: 'enhanced image'\n", "prob": 0.9}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Depth Estimation tool with input: 'enhanced image' and output: 'depth map'\n", "prob": 0.85}, "5": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "", "prob": 0.8}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'edited thumbnail image' and output: 'enhanced audio recording'\n", "prob": 0.95}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Audio-to-Audio tool with input: 'enhanced audio recording' and output: 'noisy audio recording'\n", "prob": 0.9}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "", "prob": 0.85}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'edited thumbnail image' and output: 'enhanced image'\n", "prob": 0.95}, "10": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Depth Estimation tool with input: 'enhanced image' and output: 'depth map'\n", "prob": 0.9}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Text-to-Speech tool with input: 'Enhanced image description' and output: 'enhanced audio description'\n", "prob": 0.95}, "12": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Audio-to-Audio tool with input: 'noisy audio recording' and output: 'enhanced audio recording'\n", "prob": 0.9}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Automatic Speech Recognition tool with input: 'enhanced audio description' and output: 'transcribed text'\n", "prob": 0.95}, "14": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Text-to-Image tool with input: 'transcribed text' and output: 'modified image'\n", "prob": 0.9}, "15": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Image Editing tool with input: 'modified image' and output: 'final image'\n", "prob": 0.95}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Depth Estimation tool with input: 'final image' and output: 'depth map'\n", "prob": 0.9}, "17": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Visual Question Answering tool with input: 'final image' and output: 'question answer'\n", "prob": 0.95}, "18": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Text-to-Speech tool with input: 'question answer' and output: 'question answer audio'\n", "prob": 0.95}, "19": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Text-to-Video tool with input: 'question answer audio' and output: 'question answer video'\n", "prob": 0.9}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a noisy audio recording 'example.wav' and I would like to enhance the audio and separate the sources, transcribe the enhanced audio into text, and then use that text to modify an image 'example.jpg' accordingly. Once the image is modified, I want to estimate the depth of the objects in the image. Then, I have a question 'What is the color of the main object in the modified image?'. Answer this question and generate a video 'example.mp4' based on the answer.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Image Editing tool with input: 'edited thumbnail image' and output: 'enhanced image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [2, 5], [2, 6], [3, 7], [3, 8], [3, 9], [4, 10], [4, 11]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Tool Call of Step 2:Call Image-to-Text tool with input: 'example.jpg' and output: 'extracted text from the image'\n", "prob": 0.95}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Tool Call of Step 2:Call Sentence Similarity tool with input: ['extracted text from the image', 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds'] and output: 'similarity score'\n", "prob": 0.9}, "5": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'example.jpg' and output: 'extracted_text.txt'\n", "prob": 0.95}, "6": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Speech tool with input: 'extracted_text.txt' and output: 'spoken_text.mp3'\n", "prob": 0.9}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'edited thumbnail image'\n", "prob": 0.9}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'edited thumbnail image' and output: 'final image for blog post'\n", "prob": 0.85}, "9": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 3:Call Image-to-Text tool with input: 'example.jpg' and output: 'extracted text from the image'\n", "prob": 0.95}, "11": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Sentence Similarity tool with input: '<node-0>', '<node-1>' and output: 'similarity score'\n", "prob": 0.9}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image 'example.jpg' that contains text. Please convert this image into text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Image-to-Text tool with input: 'example.jpg' and output: 'extracted text from the image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.6915149882393594, "var_p_ucb": 1.8902695340883735, "action": "\n", "prob": 1.0}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "2": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "5": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "", "prob": 0.7}, "6": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "9": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "10": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "11": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long article about the history and benefits of meditation. I would like to know the best time to meditate according to the article and have a conversation and paraphrasing of the response.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \nTool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 7], [2, 8], [2, 9], [2, 10], [3, 11], [3, 12], [3, 13], [3, 14], [4, 15], [4, 16], [4, 17]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.6915149882393594, "var_p_ucb": 1.8902695340883735, "action": "\n", "prob": 1.0}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "2": {"ucb": 2.537272482359039, "p_ucb": 2.537272482359039, "var_p_ucb": 2.8354043011325603, "action": "\n", "prob": 1.0}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.9914794405395895, "var_p_ucb": 2.158998825925469, "action": "Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'class label'\n", "prob": 0.95}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.773329466285538, "var_p_ucb": 4.090734617542994, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "5": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "6": {"ucb": 4.19258829587282, "p_ucb": 3.3540706366982564, "var_p_ucb": 3.636208548927106, "action": "", "prob": 0.8}, "7": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'class label'\n", "prob": 0.95}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "9": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "10": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "", "prob": 0.8}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.163707522399251, "var_p_ucb": 3.3712184716753884, "action": "Tool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'class label'\n", "prob": 0.95}, "12": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "13": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "14": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "", "prob": 0.8}, "15": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 3:Call Image Classification tool with input: 'example.jpg' and output: 'class label'\n", "prob": 0.9}, "16": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 3:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.85}, "17": {"ucb": 3.3302184446307908, "p_ucb": 2.6641747557046327, "var_p_ucb": 2.8389208182529586, "action": "Tool Call of Step 3:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.8}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \n", "value": 0.0}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have an image named 'example.jpg', and I want to know which class the image belongs to according to an image classification model.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    \nTool Call of Step 2:Call Image Classification tool with input: 'example.jpg' and output: 'class label'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "5": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "", "prob": 0.7}, "6": {"ucb": 3.3302184446307908, "p_ucb": 2.9971966001677117, "var_p_ucb": 3.193785920534579, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "7": {"ucb": 3.3302184446307908, "p_ucb": 2.830685677936172, "var_p_ucb": 3.0163533693937685, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "8": {"ucb": 3.3302184446307908, "p_ucb": 2.331152911241553, "var_p_ucb": 2.484055715971339, "action": "", "prob": 0.7}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a voice recording 'example.wav' and an image 'example.jpg'. Can you first detect the emotion of the narrator from the audio, then answer the question 'How may this person feel in this scene?' based on the image and detected emotion. After that, generate a brief text describing the answer, and finally, identify the named entities and part-of-speech in the text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}}]
[[[0, 0], [0, 1], [0, 2], [1, 3], [1, 4], [1, 5], [2, 6], [2, 7], [2, 8], [3, 9], [3, 10], [3, 11], [4, 12], [4, 13], [4, 14]], [[0, 1], [1, 2], [2, 3], [3, 4]], {"0": {"ucb": 1.6915149882393594, "p_ucb": 1.5223634894154234, "var_p_ucb": 1.7012425806795364, "action": "Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "1": {"ucb": 2.537272482359039, "p_ucb": 2.156681610005183, "var_p_ucb": 2.4100936559626764, "action": "Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "2": {"ucb": 2.537272482359039, "p_ucb": 1.7760907376513273, "var_p_ucb": 1.9847830107927924, "action": "", "prob": 0.7}, "3": {"ucb": 2.09629414793641, "p_ucb": 1.886664733142769, "var_p_ucb": 2.045367308771497, "action": "Tool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "prob": 0.9}, "4": {"ucb": 4.19258829587282, "p_ucb": 3.5637000514918973, "var_p_ucb": 3.8634715832350497, "action": "Tool Call of Step 2:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "prob": 0.85}, "5": {"ucb": 4.19258829587282, "p_ucb": 2.934811807110974, "var_p_ucb": 3.1816824803112174, "action": "", "prob": 0.7}, "6": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "7": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "8": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "9": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "10": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "11": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "12": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "13": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}, "14": {"ucb": 3.3302184446307908, "p_ucb": 3.3302184446307908, "var_p_ucb": 3.5486510228161983, "action": "\n", "prob": 1.0}}, {"0": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long text document about climate change, and I need a summarized version of it to present to my colleagues. Text: 'Climate change is a long-term alteration in the global climate patterns, affecting a wide array of ecosystems and human activities. The primary cause of climate change is the increase in greenhouse gas emissions primarily due to human activities such as burning fossil fuels, deforestation, and industrial processes. Climate change has resulted in rising temperatures and fluctuating weather patterns, leading to devastating natural disasters such as floods, heatwaves, and droughts. To mitigate the impacts of climate change, it is essential to adopt measures like reducing greenhouse gas emissions, shifting to renewable energy sources, and conserving forests and other natural resources.' Please provide a summarized version of this text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "1": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long text document about climate change, and I need a summarized version of it to present to my colleagues. Text: 'Climate change is a long-term alteration in the global climate patterns, affecting a wide array of ecosystems and human activities. The primary cause of climate change is the increase in greenhouse gas emissions primarily due to human activities such as burning fossil fuels, deforestation, and industrial processes. Climate change has resulted in rising temperatures and fluctuating weather patterns, leading to devastating natural disasters such as floods, heatwaves, and droughts. To mitigate the impacts of climate change, it is essential to adopt measures like reducing greenhouse gas emissions, shifting to renewable energy sources, and conserving forests and other natural resources.' Please provide a summarized version of this text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}, "2": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long text document about climate change, and I need a summarized version of it to present to my colleagues. Text: 'Climate change is a long-term alteration in the global climate patterns, affecting a wide array of ecosystems and human activities. The primary cause of climate change is the increase in greenhouse gas emissions primarily due to human activities such as burning fossil fuels, deforestation, and industrial processes. Climate change has resulted in rising temperatures and fluctuating weather patterns, leading to devastating natural disasters such as floods, heatwaves, and droughts. To mitigate the impacts of climate change, it is essential to adopt measures like reducing greenhouse gas emissions, shifting to renewable energy sources, and conserving forests and other natural resources.' Please provide a summarized version of this text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\n", "value": 0.0}, "3": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long text document about climate change, and I need a summarized version of it to present to my colleagues. Text: 'Climate change is a long-term alteration in the global climate patterns, affecting a wide array of ecosystems and human activities. The primary cause of climate change is the increase in greenhouse gas emissions primarily due to human activities such as burning fossil fuels, deforestation, and industrial processes. Climate change has resulted in rising temperatures and fluctuating weather patterns, leading to devastating natural disasters such as floods, heatwaves, and droughts. To mitigate the impacts of climate change, it is essential to adopt measures like reducing greenhouse gas emissions, shifting to renewable energy sources, and conserving forests and other natural resources.' Please provide a summarized version of this text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    ", "value": null}, "4": {"state": "\n    You are tasked with breaking down a complex user request into solvable sub-tasks by creating a task plan.\n\n    Problem Description:\n\n    # TASK LIST #:\n{\"id\": \"Token Classification\", \"desc\": \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Translation\", \"desc\": \"Translation is the task of converting text from one language to another.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Summarization\", \"desc\": \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Question Answering\", \"desc\": \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.\", \"input-type\": [\"text\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Conversational\", \"desc\": \"Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text Generation\", \"desc\": \"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.\", \"input-type\": [\"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Sentence Similarity\", \"desc\": \"Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.\", \"input-type\": [\"text\", \"text\"], \"output-type\": []}\n{\"id\": \"Tabular Classification\", \"desc\": \"Tabular classification is the task of classifying a table (in Image format).\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Object Detection\", \"desc\": \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Classification\", \"desc\": \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image-to-Image\", \"desc\": \"Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Image-to-Text\", \"desc\": \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\", \"input-type\": [\"image\"], \"output-type\": [\"text\"]}\n{\"id\": \"Text-to-Image\", \"desc\": \"Generates images from input text. These models can be used to generate images based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Video\", \"desc\": \"Generates videos from input text. These models can be used to generate videos based on text prompts.\", \"input-type\": [\"text\"], \"output-type\": [\"video\"]}\n{\"id\": \"Visual Question Answering\", \"desc\": \"Visual Question Answering is the task of answering questions based on an image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Document Question Answering\", \"desc\": \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\", \"input-type\": [\"image\", \"text\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Segmentation\", \"desc\": \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Depth Estimation\", \"desc\": \"Depth estimation is the task of predicting depth of the objects present in an image.\", \"input-type\": [\"image\"], \"output-type\": [\"image\"]}\n{\"id\": \"Text-to-Speech\", \"desc\": \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\", \"input-type\": [\"text\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Automatic Speech Recognition\", \"desc\": \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Audio-to-Audio\", \"desc\": \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\", \"input-type\": [\"audio\"], \"output-type\": [\"audio\"]}\n{\"id\": \"Audio Classification\", \"desc\": \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\", \"input-type\": [\"audio\"], \"output-type\": [\"text\"]}\n{\"id\": \"Image Editing\", \"desc\": \"Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.\", \"input-type\": [\"text\", \"image\"], \"output-type\": [\"image\"]}\n\n        \n# GOAL #: Based on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. \n        The format must in a strict JSON format, like: {\"task_steps\": [one or more concrete steps, format as Step x: step description], \"task_nodes\": [{\"task\": \"tool name must be from # TASK LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]} \n        \n        # REQUIREMENTS #: \n        1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #;\n\n        2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes;\n\n        3. the dependencies among task steps should align with the argument dependencies of the task nodes;\n\n        4. the tool arguments should be align with the input-type field of # TASK LIST #;\n\n        5. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\n\n        \nHere are provided examples for your reference.\n\n# EXAMPLE #:\n# USER REQUEST #: I have an image example.jpg and a question: 'How many people are in the image?'. Please identify the objects in the image, answer the question, and then determine the similarity between the object identification and the answered question.\n# RESULT #: ```json\n{\"task_steps\": [\"Step 1: Use Object Detection to identify objects in the given input image\", \"Step 2: Use Visual Question Answering to answer questions based on the image\", \"Step 3: Compare the generated text from Object Detection and Visual Question Answering using Sentence Similarity to determine their similarity\"], \"task_nodes\": [{\"task\": \"Object Detection\", \"arguments\": [\"example.jpg\"]}, {\"task\": \"Sentence Similarity\", \"arguments\": [\"<node-0>\", \"<node-2>\"]}, {\"task\": \"Visual Question Answering\", \"arguments\": [\"example.jpg\", \"How many people are in the image?\"]}], \"task_links\": [{\"source\": \"Object Detection\", \"target\": \"Sentence Similarity\"}, {\"source\": \"Visual Question Answering\", \"target\": \"Sentence Similarity\"}]}\n```\n    \n\n\n    # USER REQUEST #: I have a long text document about climate change, and I need a summarized version of it to present to my colleagues. Text: 'Climate change is a long-term alteration in the global climate patterns, affecting a wide array of ecosystems and human activities. The primary cause of climate change is the increase in greenhouse gas emissions primarily due to human activities such as burning fossil fuels, deforestation, and industrial processes. Climate change has resulted in rising temperatures and fluctuating weather patterns, leading to devastating natural disasters such as floods, heatwaves, and droughts. To mitigate the impacts of climate change, it is essential to adopt measures like reducing greenhouse gas emissions, shifting to renewable energy sources, and conserving forests and other natural resources.' Please provide a summarized version of this text.\n    now please generate your result in a strict JSON format:\n    # RESULT #:\n\n    Below is an example of step-by-step tool calls for planning a task to decompose a complex request into sub-tasks:\n\n    ```json\n    [\n    {\"Step 1\": \"Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\"}\n    {\"Step 2\": \"Call Image Editing tool with input: 'thumbnail image' and output: 'edited thumbnail image'\"}\n    {\"Step 3\": \"Finish\"}\n    ]\n    ```\n    Based on this example, generate step-by-step tool calls to solve the given problem by breaking it down into sub-tasks and forming a connected path.\n\n    -----Tool Calls-----\n    Tool Call of Step 1:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\nTool Call of Step 2:Call Text-to-Image tool with input: 'Describe the image content in text for the blog post, including the Eiffel Tower, sky, and clouds' and output: 'thumbnail image'\n", "value": 0.0}}]
